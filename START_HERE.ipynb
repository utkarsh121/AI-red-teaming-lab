{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ”´ Welcome to the AI Red Team Lab\n### Certified AI Penetration Tester â€“ Red Team (CAIPT-RT)\n\n---\n\nWelcome, student. This lab environment is your hands-on workspace for learning how machine learning models can be attacked â€” and why understanding these attacks makes AI systems safer.\n\nYou are going to play the role of an **attacker** (red team) in each lab. You will learn four real-world attack techniques that security researchers and AI penetration testers use to test the robustness of machine learning systems.\n\n---\n\n## ğŸ“š What You Will Learn\n\n| Lab | Attack Type | Plain English Description |\n|-----|------------|---------------------------|\n| Lab 1 | **Evasion** | Trick a trained model by crafting sneaky inputs |\n| Lab 2 | **Poisoning** | Corrupt the training data before the model learns |\n| Lab 3 | **Inference** | Violate privacy by figuring out whose data trained the model |\n| Lab 4 | **Extraction** | Steal a model just by asking it questions |\n\n---\n\n## ğŸ§  How These Labs Work\n\nEach lab notebook is **self-guiding**. Every step is explained in plain English before any code appears.\n\nThroughout each lab you will see three types of prompts:\n\n- ğŸ‘€ **\"What do you see?\"** â€” asking you to observe and interpret output\n- ğŸ§ª **\"Try this\"** â€” asking you to change something and observe what happens\n- ğŸ’­ **\"Think about it\"** â€” asking you to reflect on real-world implications\n\n---\n\n## âš ï¸ About Warnings You May See\n\nWhen you run the sanity check below, you may see a warning message like this:\n\n```\nUserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n```\n\n**This is completely normal and you can ignore it.**\n\nHere is why it appears: ART (the Adversarial Robustness Toolbox) is a large toolkit that optionally supports deep learning frameworks like PyTorch for advanced features. We are not using those features in this lab â€” all four of our attack labs use simpler scikit-learn models. ART is simply letting you know those optional modules are unavailable, which does not affect anything we are doing.\n\nIf you see this warning, just continue. It will not cause any errors in the labs.\n\n---\n\n## âš ï¸ Important Rules\n\nEverything you learn in this lab is for **defensive and educational purposes only**. All attacks run entirely on your local machine against models you build yourself. No external systems are targeted.\n\n---\n\n## ğŸ—‚ï¸ Your Lab Workspace\n\n- **`datasets/`** â€” contains the data files your models will learn from\n- **`notebooks/`** â€” contains these lab notebooks (you are here)\n- **`outputs/`** â€” save your results, screenshots, and answers here\n\n---\n\n## âœ… Step 1: Run the Sanity Check Below\n\nRun the cell below to confirm all required tools are installed. If you see version numbers printed out with checkmarks, you are ready to begin. Ignore any PyTorch warning â€” see the note above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# SANITY CHECK\n# =============================================================================\n# This cell imports all the Python libraries used across the four labs.\n# If any import fails with a ModuleNotFoundError, call your instructor.\n# A UserWarning about PyTorch is normal and harmless - see the note above.\n# =============================================================================\n\nprint(\"Checking all required libraries...\")\nprint(\"=\" * 40)\n\nimport numpy\nprint(f\"numpy          : {numpy.__version__} âœ“\")\n\nimport pandas\nprint(f\"pandas         : {pandas.__version__} âœ“\")\n\nimport sklearn\nprint(f\"scikit-learn   : {sklearn.__version__} âœ“\")\n\nimport matplotlib\nprint(f\"matplotlib     : {matplotlib.__version__} âœ“\")\n\n# ART may show a PyTorch warning here - this is normal, see note above\nimport warnings\nwarnings.filterwarnings('ignore')\nimport art\nprint(f\"ART            : {art.__version__} âœ“\")\n\nprint(\"=\" * 40)\nprint(\"\")\nprint(\"All libraries loaded successfully.\")\nprint(\"You are ready to begin the labs!\")\nprint(\"\")\nprint(\"Note: If you saw a PyTorch warning above, that is normal.\")\nprint(\"It does not affect any of the four lab exercises.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ğŸ‘€ What Do You See?\n\n- If you see version numbers with âœ“ marks â€” **you are ready, move on**\n- If you see a `ModuleNotFoundError` in red â€” call your instructor\n- If you saw a yellow PyTorch warning â€” that is normal, ignore it\n\n---\n\n## ğŸš€ Step 2: Open Your First Lab\n\nClick the links below to open each lab notebook directly.\n\n- ğŸ““ [Lab 1 â€” Evasion Attack](Lab1_Evasion_Attack.ipynb)\n- ğŸ““ [Lab 2 â€” Poisoning Attack](Lab2_Poisoning_Attack.ipynb)\n- ğŸ““ [Lab 3 â€” Inference Attack](Lab3_Inference_Attack.ipynb)\n- ğŸ““ [Lab 4 â€” Extraction Attack](Lab4_Extraction_Attack.ipynb)\n\n---\n\n## ğŸ’­ Before You Begin â€” Think About This\n\nMachine learning models are being used today to make decisions about loan approvals, medical diagnoses, fraud detection, and hiring.\n\nAs you work through these labs, keep asking yourself: **what would happen if this attack was used in the real world against one of those systems?**\n\nThat question is the whole point of this course.\n\n---\n*Lab environment built with the Adversarial Robustness Toolbox (ART) by IBM/Trusted-AI*\n*https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
  }
 ]
}