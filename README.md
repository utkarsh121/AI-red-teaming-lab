# AI Red Teaming Lab

**AI Red Teaming Lab** is a hands-on educational project designed to teach foundational and advanced concepts in AI security through executable lab notebooks and automated setup scripts.

This lab walks users through real adversarial attack techniques ‚Äî including evasion, poisoning, inference, and extraction ‚Äî on machine learning models and demonstrates how to use tools like the **Adversarial Robustness Toolbox (ART)** to explore vulnerabilities in AI systems.

---

## ‚öôÔ∏è Installation

### Requirements

The lab is intended to be used on a Unix-compatible development environment (e.g., Ubuntu). The provided installer scripts will:

* Install Python, pip, and JupyterLab
* Create a Python virtual environment
* Download required libraries (scikit-learn, numpy, pandas, matplotlib, ART, etc.)
* Set up datasets and execute Jupyter notebooks

### Run Installation

```bash
bash lab_installer.sh
```

This script will check for `curl`, download the full installation script (`lab_installer_main.sh`), and begin environment setup.

---

## üìÅ Project Structure

```
AI-red-teaming-lab/
‚îú‚îÄ‚îÄ lab_installer.sh
‚îú‚îÄ‚îÄ lab_installer_main.sh
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ START_HERE.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Lab1_Evasion_Attack.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Lab2_Poisoning_Attack.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Lab3_Inference_Attack.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Lab4_Extraction_Attack.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ sms_spam_collection_dataset.csv
‚îÇ   ‚îî‚îÄ‚îÄ nursery_dataset.csv
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îî‚îÄ‚îÄ charts_and_results/
‚îî‚îÄ‚îÄ README.md
```

---

## üìò Notebooks Overview

### ü™™ START_HERE.ipynb

**Description:** Introductory notebook to guide users through setup verification and how to work with the lab environment.

**Learning Goals:**

* Confirm environment is working
* Launch JupyterLab
* Understand navigation between notebooks

---

### üß† Lab 1: **Evasion Attack**

**File:** `Lab1_Evasion_Attack.ipynb`

**Summary:** Train a text-based classifier (e.g., SMS spam detector) and perform an **evasion attack** that perturbs inputs to cause the model to misclassify.

**Concepts Covered:**

* TF-IDF text vectorization
* Logistic regression classifier
* Introduction to adversarial attacks
* Using ART‚Äôs `HopSkipJump` attack

**Result:** Understand how small perturbations to inputs can fool deployed models.

---

### üíÄ Lab 2: **Poisoning Attack**

**File:** `Lab2_Poisoning_Attack.ipynb`

**Summary:** Demonstrates how to **poison training data** such that model behavior is corrupted when later used in production.

**Concepts Covered:**

* Training/validation dataset manipulation
* Backdoor patterns
* Targeted vs. untargeted poisoning
* Evaluating model resilience

**Result:** See how model performance shifts when poisoned data is introduced.

---

### üîç Lab 3: **Inference Attack**

**File:** `Lab3_Inference_Attack.ipynb`

**Summary:** Shows how adversaries can perform **inference attacks** to extract sensitive properties about training data.

**Concepts Covered:**

* Membership inference techniques
* Shadow models (optional)
* Risk evaluation

**Result:** Determine the level of privacy leakage from a deployed model.

---

### üß¨ Lab 4: **Extraction Attack**

**File:** `Lab4_Extraction_Attack.ipynb`

**Summary:** Practical exploration of **model extraction attacks**, where an attacker tries to replicate a target model via API queries.

**Concepts Covered:**

* Black-box querying
* Surrogate model training
* Evaluation of extraction fidelity

**Result:** Understand how models can be reverse-engineered.

---

## üìä Datasets

The `datasets/` folder contains the CSV datasets used in the labs:

* **sms_spam_collection_dataset.csv** ‚Äî SMS text and labels for spam classification
* **nursery_dataset.csv** ‚Äî Example structured dataset for learning exercises

> ‚ö†Ô∏è Make sure these are in place before running the respective notebooks.

---

## üì¶ Dependencies

The major Python dependencies installed include:

* `numpy`
* `pandas`
* `scikit-learn`
* `matplotlib`
* `adversarial-robustness-toolbox (ART)`

---

## üéØ Goals

After completing these labs, you will be able to:

* Understand common adversarial attack categories against AI models
* Set up a reproducible Python security testing environment
* Experiment with open-source AI adversarial tools
* Interpret attack success and model resilience metrics
* Build a foundational skillset for AI red teaming and security evaluation

---

## üìñ References

* Adversarial Machine Learning and AI Security
* Adversarial Robustness Toolbox (ART)
* MITRE ATLAS adversarial threat framework
* Prompt injection, jailbreaks, and generative model attacks

---


## ‚ú® Contributing

If you want to expand this lab:

* Add more attack categories (e.g., generative model prompt injection)
* Include defense notebooks (e.g., robust training)
* Provide evaluation challenges and grading scripts

Thank you for contributing!

---

If you **can provide a file list** from the *utkarsh121/AI-red-teaming-lab* repository or share its contents, I can tailor this README exactly to the files present.
