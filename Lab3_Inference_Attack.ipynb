{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸ”´ Lab 3 â€” Inference Attack (Membership Inference)\n### Certified AI Penetration Tester â€“ Red Team (CAIPT-RT)\n\n---\n\n## ðŸŽ¯ The Story\n\nA hospital trained a machine learning model on real patient records â€” sensitive data including age, family background, financial situation, and health history. The hospital never releases the records themselves, but they do offer the trained model as a public tool: you send it a patient profile, it sends back a risk prediction.\n\nYou are an attacker. You do not have the patient records. But you have access to the model. Can you figure out **whether a specific person's data was used to train it?**\n\nIf you can, you have violated that person's privacy â€” you now know they were a patient at this hospital and their data was part of a sensitive medical study. That is a real legal and ethical violation even if you never saw the actual record.\n\nThis is called a **Membership Inference Attack**.\n\n---\n\n## ðŸ“– What is a Membership Inference Attack?\n\nIt tries to determine whether a specific data point was part of a model's training dataset.\n\nThink of it this way: imagine a chef who memorised certain recipes by cooking them over and over. If you asked that chef to recreate a dish, they would do it much more accurately for dishes they personally practised than for dishes they only vaguely know. A smart observer could figure out which dishes the chef trained on just by watching how confident they are.\n\nMachine learning models behave the same way. They tend to be **more confident and make fewer mistakes on data they trained on** compared to data they have never seen. An attacker exploits this subtle difference.\n\n**Real world examples:**\n- Determining if a specific person's medical record was in a clinical trial dataset\n- Confirming if someone's financial data trained a credit scoring model\n- Violating GDPR or HIPAA by inferring membership in sensitive datasets\n\n---\n\n## ðŸ—‚ï¸ What We Will Do in This Lab\n\n1. Load the Nursery dataset â€” a realistic dataset of private family records\n2. Prepare and encode the data so the model can understand it\n3. Train a classifier and measure the gap between training and test performance\n4. Run a simple rule-based membership inference attack\n5. Run a more sophisticated black-box attack that trains its own attack model\n6. Understand why the gap between training and test accuracy is the root cause\n\n---\n\n## âš™ï¸ Step 1: Import the Tools We Need"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# IMPORTS\n# =============================================================================\n# numpy      : handles numbers and arrays â€” the fundamental math library\n# pandas     : loads and manages datasets like a spreadsheet in Python\n# matplotlib : draws charts and graphs\n# sklearn    : builds and trains machine learning models\n# art        : Adversarial Robustness Toolbox â€” the attack toolkit\n#\n# We suppress the PyTorch warning from ART. PyTorch is an optional\n# deep learning framework that we are not using in this lab.\n# =============================================================================\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom art.estimators.classification import SklearnClassifier\nfrom art.attacks.inference.membership_inference import (\n    MembershipInferenceBlackBox,\n    MembershipInferenceBlackBoxRuleBased\n)\nfrom art.utils import to_categorical\n\nnp.random.seed(42)\nprint(\"All tools imported successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ðŸ“‚ Step 2: Load and Understand the Nursery Dataset\n\nWe are using the **Nursery dataset** from the UCI Machine Learning Repository. It was originally created to rank applications for nursery school enrollment in the 1980s in Ljubljana, Slovenia.\n\nEach row in this dataset represents a **family application**. The columns capture private information about each family â€” their financial situation, housing conditions, number of children, health status, and social circumstances.\n\nFor our attack scenario, imagine each row is a patient record at a hospital, and the model was trained on these records to make enrollment or eligibility decisions. The records are private. The model is public.\n\n**Why this dataset?** Because it has real categorical structure (words, not just numbers), multiple classes to predict, and enough rows (nearly 13,000) to make a realistic attack experiment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LOAD THE NURSERY DATASET\n# =============================================================================\n# The file has no header row â€” we supply the column names ourselves\n# based on the UCI dataset documentation.\n#\n# Each row represents one family application with 8 features and a target.\n# =============================================================================\n\ncolumn_names = [\n    'parents',   # Parents' occupation: usual, pretentious, great_pret\n    'has_nurs',  # Quality of the nursery: proper, less_proper, improper, critical, very_crit\n    'form',      # Family form: complete, completed, incomplete, foster\n    'children',  # Number of children: 1, 2, 3, more\n    'housing',   # Housing conditions: convenient, less_conv, critical\n    'finance',   # Financial standing: convenient, inconv\n    'social',    # Social conditions: nonprob, slightly_prob, problematic\n    'health',    # Health conditions: recommended, priority, not_recom\n    'target'     # Enrollment decision: recommend, priority, not_recom, very_recom, spec_prior\n]\n\ndf = pd.read_csv(\n    '../datasets/nursery.data',\n    header=None,\n    names=column_names\n)\n\nprint(f\"Dataset loaded successfully.\")\nprint(f\"Total records : {len(df)}\")\nprint(f\"Total columns : {len(df.columns)}\")\nprint(\"\")\nprint(\"First 5 records:\")\nprint(\"-\" * 70)\nprint(df.head().to_string())\nprint(\"\")\nprint(\"How many applications received each decision:\")\nprint(df['target'].value_counts().to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\n- Each row is a family. Each column is a piece of private information about them.\n- The `target` column is what the model learned to predict â€” the enrollment decision.\n- Notice that most applications were marked `not_recom` (not recommended). This imbalance matters â€” a model that just always guesses `not_recom` would be very accurate but completely useless.\n\n**Think about this:** In a real hospital scenario, each of these rows might be a patient. The `target` might be a diagnosis. If an attacker could determine that a specific person's row was in this dataset, they would know that person received medical care at this institution â€” which is private information.\n\n---\n\n## ðŸ”¢ Step 3: Convert Text to Numbers (Encoding)\n\nMachine learning models cannot read words. They only understand numbers. Every piece of text in this dataset must be converted into a number before training can begin.\n\nWe use a technique called **Label Encoding**. It assigns a unique integer to each unique text value in a column. For example:\n\n- `finance` column: `convenient` â†’ 0, `inconv` â†’ 1\n- `health` column: `not_recom` â†’ 0, `priority` â†’ 1, `recommended` â†’ 2\n\nThis is purely a technical translation â€” it does not change the meaning of the data, it just puts it in a form the model can do mathematics with.\n\n**Important:** After encoding, `X` is the input data (the 8 feature columns) and `y` is the target (the enrollment decision we want to predict)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LABEL ENCODING â€” CONVERT TEXT TO NUMBERS\n# =============================================================================\n# LabelEncoder.fit_transform() does two things in one step:\n#   fit()      : scans the column and finds all unique values\n#   transform(): replaces each unique value with its assigned integer\n#\n# We apply this to every column including the target column.\n# =============================================================================\n\ndf_encoded = df.copy()\nencoders = {}\n\nfor column in df_encoded.columns:\n    le = LabelEncoder()\n    df_encoded[column] = le.fit_transform(df_encoded[column])\n    encoders[column] = le\n\nX = df_encoded.drop('target', axis=1).values  # 8 input features as a 2D array\ny = df_encoded['target'].values                # target labels as a 1D array\n\nprint(\"Encoding complete.\")\nprint(\"\")\nprint(f\"X shape : {X.shape}\")\nprint(f\"  -> {X.shape[0]} records, each described by {X.shape[1]} numbers\")\nprint(\"\")\nprint(f\"y shape : {y.shape}\")\nprint(f\"  -> {y.shape[0]} labels, one per record\")\nprint(\"\")\nprint(f\"Unique target classes in y: {np.unique(y)}\")\nprint(f\"That represents {len(np.unique(y))} possible decisions the model can make\")\nprint(\"\")\nprint(\"Example â€” first record before and after encoding:\")\nprint(f\"  Original : {df.iloc[0].to_dict()}\")\nprint(f\"  Encoded  : {df_encoded.iloc[0].to_dict()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\n- Each record is now a row of 8 numbers instead of 8 words.\n- `y` contains values 0 through 4, each representing one of the 5 enrollment decisions.\n- The encoding is purely mechanical â€” the model does not know that `not_recom` means anything, it just learns that records with certain number patterns tend to get label 0.\n\n---\n\n## âœ‚ï¸ Step 4: Split Data into Training and Testing Sets\n\nBefore training the model, we split our data into two groups:\n\n**Training set** â€” the records the model learns from. In our attack scenario, we call these the **members**. These are the private records that went into the model.\n\n**Test set** â€” records the model has never seen during training. We call these the **non-members**. The model has no memory of these at all.\n\nThe whole point of a membership inference attack is to figure out which group a given record belongs to â€” **without ever seeing the split directly**. We only have the model and the ability to query it.\n\n**Why `stratify=y`?** This ensures that the same proportion of each target class appears in both the training and test sets. Without this, by pure chance you might end up with all the rare `very_recom` records landing in training and none in the test set, which would make the split unrepresentative."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAIN / TEST SPLIT\n# =============================================================================\n# test_size=0.3  : 70% of records go to training, 30% to testing\n# random_state=42: sets the random seed so everyone gets the same split\n# stratify=y     : ensures class proportions are the same in both sets\n#\n# TERMINOLOGY FOR THIS LAB:\n#   X_train / y_train = MEMBERS   (the model will learn from these)\n#   X_test  / y_test  = NON-MEMBERS (the model never sees these)\n# =============================================================================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Data split complete.\")\nprint(\"\")\nprint(f\"MEMBERS (training set)     : {len(X_train):,} records\")\nprint(f\"NON-MEMBERS (test set)     : {len(X_test):,} records\")\nprint(\"\")\n\n# Verify the class balance is preserved in both sets\nprint(\"Class distribution check (should be similar in both sets):\")\nprint(f\"  Training - unique classes: {np.unique(y_train)}  counts: {[sum(y_train==c) for c in np.unique(y_train)]}\")\nprint(f\"  Testing  - unique classes: {np.unique(y_test)}   counts: {[sum(y_test==c) for c in np.unique(y_test)]}\")\nprint(\"\")\nprint(\"The attack will try to figure out which set each record belongs to,\")\nprint(\"using only the model's behaviour â€” not this split directly.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ðŸ‹ï¸ Step 5: Train the Target Model\n\nWe train a **Random Forest** classifier. A Random Forest is an ensemble model â€” it builds many individual decision trees and combines their votes into a final prediction.\n\n**What is a decision tree?** Imagine a flowchart of yes/no questions: \"Is the family's finance status convenient? If yes, go left. If no, go right.\" Each question narrows down which enrollment category the family likely belongs to.\n\n**Why a forest instead of one tree?** One tree can memorise the training data very specifically and make poor predictions on new data. Having hundreds of trees each trained on slightly different samples, then voting on the answer, produces a much more reliable result. This is the same principle as asking 100 experts instead of 1.\n\n**The parameter `n_estimators=100`** means we build 100 trees. More trees generally means better accuracy but slower training.\n\n**The critical thing to watch:** After training, we measure accuracy on both the training set and the test set. The difference between these two numbers is what makes membership inference possible."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAIN THE TARGET MODEL (RANDOM FOREST)\n# =============================================================================\n# RandomForestClassifier builds n_estimators independent decision trees.\n# Each tree votes on the class label, and the majority vote wins.\n#\n# fit(X_train, y_train) is where training happens:\n#   - X_train : the 8-feature input vectors for each training record\n#   - y_train : the correct label (0-4) for each record\n#\n# The model adjusts its internal structure until it can map inputs to labels\n# correctly. This is the learning step.\n# =============================================================================\n\nprint(\"Training target model (100 decision trees)...\")\nprint(\"This may take 15-20 seconds.\")\nprint(\"\")\n\ntarget_model = RandomForestClassifier(n_estimators=100, random_state=42)\ntarget_model.fit(X_train, y_train)\n\n# Measure accuracy on training data (members)\ntrain_accuracy = accuracy_score(y_train, target_model.predict(X_train))\n\n# Measure accuracy on test data (non-members â€” never seen during training)\ntest_accuracy = accuracy_score(y_test, target_model.predict(X_test))\n\ngap = train_accuracy - test_accuracy\n\nprint(\"Training complete!\")\nprint(\"\")\nprint(\"=\" * 55)\nprint(f\"Accuracy on MEMBERS (training data)   : {train_accuracy*100:.2f}%\")\nprint(f\"Accuracy on NON-MEMBERS (test data)   : {test_accuracy*100:.2f}%\")\nprint(f\"Gap                                   : {gap*100:.2f}%\")\nprint(\"=\" * 55)\nprint(\"\")\nprint(\"What this gap means:\")\nprint(\"  The model performs better on records it has seen before.\")\nprint(\"  This is because it has partially memorised them during training.\")\nprint(\"  An attacker can exploit this difference â€” if the model is more\")\nprint(\"  confident and accurate on a record, it was probably a member.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\nThe model performs better on training data (members) than on test data (non-members). This is called **overfitting** â€” the model has partially memorised its training examples rather than just learning general patterns.\n\n**Why does overfitting enable attacks?**\n\nThink of it like a student who memorised last year's exam answers word for word. Ask them a question from last year's exam and they answer instantly with high confidence. Ask them a new question on the same topic and they are less sure. A smart observer watching this student could figure out which questions came from the old exam just by watching their confidence level.\n\nThe model behaves the same way:\n- Member record â†’ model predicts with high confidence â†’ likely correct\n- Non-member record â†’ model is slightly less sure â†’ slightly more likely to be wrong\n\nThe attack exploits this signal.\n\n**The larger the gap, the easier the attack.** A perfectly generalising model with zero gap would give the attacker nothing to work with.\n\n---\n\n## ðŸ”´ Step 6: Rule-Based Membership Inference Attack\n\nThe first attack is beautifully simple. It uses just one rule:\n\n> **If the model predicts the correct label for a record â†’ guess it is a member.**\n> **If the model predicts incorrectly â†’ guess it is a non-member.**\n\nThis works because models are more accurate on data they trained on. It is not a perfect attack â€” the model does sometimes get non-member records right too â€” but it performs better than random guessing, which is the baseline we care about.\n\n**Why is 50% the baseline?** Because random guessing on a binary question (member vs non-member) gives 50% accuracy by definition. Any attack that scores above 50% is learning something real about the model's behaviour. Even 55% accuracy is a privacy violation in a sensitive context â€” it means the attacker is gaining real information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RULE-BASED MEMBERSHIP INFERENCE ATTACK\n# =============================================================================\n# This attack implements one simple rule:\n#   correct prediction â†’ infer \"member\"\n#   wrong prediction   â†’ infer \"non-member\"\n#\n# We wrap our model in ART's SklearnClassifier so ART can work with it.\n#\n# IMPORTANT â€” why to_categorical()?\n# ART's infer() function internally converts labels to one-hot format.\n# One-hot means: for 5 classes, label 3 becomes [0, 0, 0, 1, 0].\n# ART infers the number of classes from the sample passed in â€” but if\n# our sample of 200 happens to be missing one class by chance, ART\n# builds an array that is too small, and crashes with an IndexError.\n#\n# The fix: we convert to one-hot ourselves first, using the FULL class\n# count from the complete dataset. This way ART always sees the right size.\n# =============================================================================\n\nart_model   = SklearnClassifier(model=target_model)\nrule_attack = MembershipInferenceBlackBoxRuleBased(art_model)\n\n# nb_classes = total number of unique target classes in the full dataset\n# We compute this from y (full dataset) NOT from the sample\nnb_classes  = len(np.unique(y))\nsample_size = 200\n\nprint(f\"Total classes in dataset : {nb_classes}\")\nprint(f\"Sample size for attack   : {sample_size} members + {sample_size} non-members\")\nprint(\"\")\n\n# Draw a random sample of members (from training set)\nmember_idx = np.random.choice(len(X_train), sample_size, replace=False)\nX_member   = X_train[member_idx]\ny_member   = y_train[member_idx]\n\n# Draw a random sample of non-members (from test set)\nnonmember_idx = np.random.choice(len(X_test), sample_size, replace=False)\nX_nonmember   = X_test[nonmember_idx]\ny_nonmember   = y_test[nonmember_idx]\n\nprint(f\"Classes present in member sample    : {np.unique(y_member)}\")\nprint(f\"Classes present in non-member sample: {np.unique(y_nonmember)}\")\nprint(\"\")\n\n# Convert labels to one-hot using FULL class count (the key fix)\n# to_categorical(labels, nb_classes) turns integer labels into binary rows\n# Example with 5 classes: label 2 â†’ [0, 0, 1, 0, 0]\ny_member_oh    = to_categorical(y_member,    nb_classes)\ny_nonmember_oh = to_categorical(y_nonmember, nb_classes)\n\nprint(f\"One-hot shape for members    : {y_member_oh.shape}  (200 records Ã— {nb_classes} classes)\")\nprint(f\"One-hot shape for non-members: {y_nonmember_oh.shape}\")\nprint(\"\")\n\n# Run the attack\n# infer() returns 1 for each record it thinks is a member, 0 for non-member\nmember_inferred    = rule_attack.infer(X_member,    y_member_oh)\nnonmember_inferred = rule_attack.infer(X_nonmember, y_nonmember_oh)\n\n# How often did it correctly identify members as members?\nmember_accuracy    = np.mean(member_inferred == 1)\n# How often did it correctly identify non-members as non-members?\nnonmember_accuracy = np.mean(nonmember_inferred == 0)\n# Overall = average of both\noverall_accuracy   = (member_accuracy + nonmember_accuracy) / 2\n\nprint(\"Rule-Based Attack Results:\")\nprint(\"=\" * 55)\nprint(f\"Correctly identified members     : {member_accuracy*100:.1f}%\")\nprint(f\"Correctly identified non-members : {nonmember_accuracy*100:.1f}%\")\nprint(f\"Overall attack accuracy          : {overall_accuracy*100:.1f}%\")\nprint(\"\")\nprint(f\"Random guessing baseline         : 50.0%\")\nprint(f\"Advantage over random guessing   : +{(overall_accuracy - 0.5)*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\n- **Random guessing gives 50%.** Any score above this means the attack is learning real information.\n- Even a small advantage â€” say 55% or 60% â€” is meaningful in privacy terms. If someone's medical record was in a hospital's AI training set, you now have better-than-chance odds of knowing that.\n- The rule-based attack is simple and fast but limited. It only uses whether the prediction was right or wrong â€” it ignores *how* confident the model was.\n\n### ðŸ§ª Try This\n\nAfter running the rule-based attack, think about this: what if the model was equally accurate on members and non-members? The rule would not work at all. This is why privacy researchers recommend training models that generalise well â€” the attack surface shrinks as the accuracy gap shrinks.\n\n---\n\n## ðŸ”´ Step 7: Black-Box Membership Inference Attack\n\nThe second attack is more sophisticated. Instead of a simple rule, it **trains its own small model** â€” called the attack model â€” to distinguish members from non-members.\n\n**How does it work?**\n\nThe key insight is that when a model predicts on a record, it does not just output one label â€” it outputs a **probability distribution** across all possible classes. For example: `[0.02, 0.01, 0.91, 0.03, 0.03]`. This says the model is 91% confident the record belongs to class 2.\n\nMembers tend to get **higher confidence scores** on their true class. Non-members tend to get more spread-out, uncertain distributions. The attack model learns to tell these two patterns apart.\n\n**This is called a black-box attack** because the attacker never sees inside the victim model â€” no code, no weights, no training data. They only see inputs and outputs, exactly as if they were calling a real API.\n\n**The attack model is trained in two phases:**\n1. Query the victim model on known members and known non-members to collect (confidence, true label) pairs\n2. Train a small classifier to predict member/non-member from those confidence patterns\n\nThe attacker then uses this trained attack model to classify unknown records."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# BLACK-BOX MEMBERSHIP INFERENCE ATTACK\n# =============================================================================\n# MembershipInferenceBlackBox works in two phases:\n#\n# Phase 1 â€” fit():\n#   - Takes some known members and known non-members\n#   - Queries the victim model on all of them\n#   - Collects the victim's output probabilities for each record\n#   - Trains a small Random Forest (the attack model) to learn:\n#     high confidence on true class â†’ likely member\n#     lower, more uncertain confidence â†’ likely non-member\n#\n# Phase 2 â€” infer():\n#   - Takes new records of unknown membership\n#   - Queries the victim model for their probabilities\n#   - Feeds those probabilities to the trained attack model\n#   - Returns 1 (member) or 0 (non-member) for each record\n#\n# The key fix applied here is the same as above: we convert labels to\n# one-hot using the FULL class count before passing them to ART, to\n# prevent the IndexError caused by ART miscounting classes from a sample.\n# =============================================================================\n\nprint(\"Running black-box membership inference attack...\")\nprint(\"Phase 1: Training the attack model on known members and non-members...\")\nprint(\"(May take 15-30 seconds)\")\nprint(\"\")\n\nbb_attack = MembershipInferenceBlackBox(art_model, attack_model_type='rf')\n\n# We use the first half of our samples to TRAIN the attack model\n# and the second half to TEST it (evaluate how well it generalised)\ntrain_split = sample_size // 2  # 100 for training, 100 for testing\n\n# Convert all four label arrays to one-hot using the FULL class count\n# Each of these will be (100, 5) arrays â€” 100 records Ã— 5 classes\ny_member_train_oh    = to_categorical(y_member[:train_split],    nb_classes)\ny_nonmember_train_oh = to_categorical(y_nonmember[:train_split], nb_classes)\ny_member_test_oh     = to_categorical(y_member[train_split:],    nb_classes)\ny_nonmember_test_oh  = to_categorical(y_nonmember[train_split:], nb_classes)\n\nprint(f\"Attack model training: {train_split} members + {train_split} non-members\")\nprint(f\"Attack model testing : {train_split} members + {train_split} non-members (held out)\")\nprint(f\"Label array shapes   : {y_member_train_oh.shape} each\")\nprint(\"\")\n\n# fit() trains the attack model on the known labelled examples\nbb_attack.fit(\n    x      = X_member[:train_split],        # known member inputs\n    y      = y_member_train_oh,             # their one-hot labels\n    x_test = X_nonmember[:train_split],     # known non-member inputs\n    y_test = y_nonmember_train_oh           # their one-hot labels\n)\n\nprint(\"Phase 2: Using the trained attack model to classify held-out records...\")\nprint(\"\")\n\n# infer() uses the attack model on the held-out second half\nbb_member_inferred    = bb_attack.infer(X_member[train_split:],    y_member_test_oh)\nbb_nonmember_inferred = bb_attack.infer(X_nonmember[train_split:], y_nonmember_test_oh)\n\nbb_member_acc    = np.mean(bb_member_inferred == 1)\nbb_nonmember_acc = np.mean(bb_nonmember_inferred == 0)\nbb_overall       = (bb_member_acc + bb_nonmember_acc) / 2\n\nprint(\"Black-Box Attack Results:\")\nprint(\"=\" * 55)\nprint(f\"Correctly identified members     : {bb_member_acc*100:.1f}%\")\nprint(f\"Correctly identified non-members : {bb_nonmember_acc*100:.1f}%\")\nprint(f\"Overall attack accuracy          : {bb_overall*100:.1f}%\")\nprint(\"\")\nprint(f\"Random guessing baseline         : 50.0%\")\nprint(f\"Advantage over random guessing   : +{(bb_overall - 0.5)*100:.1f}%\")\nprint(\"\")\nprint(\"Comparison:\")\nprint(f\"  Rule-based attack    : {overall_accuracy*100:.1f}%\")\nprint(f\"  Black-box attack     : {bb_overall*100:.1f}%\")\nprint(f\"  Improvement          : +{(bb_overall - overall_accuracy)*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\n- The black-box attack uses richer information â€” the full probability distribution â€” rather than just whether the prediction was right or wrong. This usually produces a better result.\n- Compare the two attacks. How much did the extra sophistication help?\n\n**On confidence distributions â€” why do they leak membership?**\n\nWhen a model trains on a record it tends to push its confidence for the correct class very high â€” sometimes to 0.95 or 0.99. On a non-member record it has never seen, the confidence is more spread out. Even a small systematic difference across many records is enough for the attack model to learn the pattern.\n\n### ðŸ§ª Try This\n\nGo back to the model training step and change `n_estimators=100` to `n_estimators=500`. Retrain and rerun the attacks.\n\n- Does a more powerful model with less overfitting become harder to attack?\n- What does this tell you about the relationship between model quality and privacy risk?\n\n---\n\n## ðŸ’­ Step 8: Reflect on What You Have Learned"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# REFLECTION â€” edit your answers below and run to save them\n# =============================================================================\n\nreflection = \"\"\"\nLAB 3 - INFERENCE ATTACK REFLECTION\n=====================================\n\nQ1: In plain English, what is a membership inference attack?\n    What does the attacker learn, and why is it a privacy violation\n    even if they never see the actual record?\nA1: [TYPE YOUR ANSWER HERE]\n\nQ2: The attack worked because the model had a training/test accuracy gap.\n    What is overfitting in plain English, and why does it create this gap?\n    Why does a larger gap make the attack easier?\nA2: [TYPE YOUR ANSWER HERE]\n\nQ3: The rule-based attack only used whether the prediction was correct.\n    The black-box attack used the full probability distribution.\n    Why does using probabilities give the attacker more information?\nA3: [TYPE YOUR ANSWER HERE]\n\nQ4: Even a small advantage above 50% is a serious privacy problem in\n    medical or financial contexts. Give one concrete real-world example\n    of harm that could result from a successful membership inference attack.\nA4: [TYPE YOUR ANSWER HERE]\n\nQ5: What defensive measures could reduce membership inference risk?\n    Think about both technical controls (differential privacy, regularisation)\n    and operational controls (who can query the model, how many times).\nA5: [TYPE YOUR ANSWER HERE]\n\nQ6: Which privacy regulation (GDPR, HIPAA, CCPA, etc.) would apply if\n    an attacker revealed that someone's medical data was in a hospital's\n    AI training set? What legal consequences could the hospital face?\nA6: [TYPE YOUR ANSWER HERE]\n\"\"\"\n\nwith open('../outputs/Lab3_Reflection.txt', 'w') as f:\n    f.write(reflection)\n\nprint(\"Reflection saved to outputs/Lab3_Reflection.txt\")\nprint(reflection)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## âœ… Lab 3 Complete\n\nYou have trained a real classifier, observed the overfitting gap, and performed two membership inference attacks â€” one simple and one sophisticated â€” using the Adversarial Robustness Toolbox.\n\nThe key lesson: **any model that behaves differently on its training data than on new data is potentially leaking privacy information**. The attacker does not need access to the training data or model internals. The model's own output probabilities are enough.\n\nReturn to [START_HERE.ipynb](START_HERE.ipynb) and open Lab 4 â€” Extraction Attack.\n\n---\n*Built with the Adversarial Robustness Toolbox (ART) â€” https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
  }
 ]
}