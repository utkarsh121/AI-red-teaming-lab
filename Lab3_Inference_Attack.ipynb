{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üî¥ Lab 3 ‚Äî Inference Attack (Membership Inference)\n### Certified AI Penetration Tester ‚Äì Red Team (CAIPT-RT)\n\n---\n\n## üéØ The Story\n\nA hospital trained a machine learning model on real patient records ‚Äî sensitive data including age, family background, financial situation, and health history. They never release the records themselves, but they do offer the trained model as a public tool: send it a patient profile, get a risk prediction back.\n\nYou are an attacker. You do not have the patient records. But you have access to the model. Can you figure out **whether a specific person's data was used to train it?**\n\nIf you can, you have violated that person's privacy ‚Äî you now know they were a patient at this hospital and their data was part of a sensitive medical study.\n\nThis is a **Membership Inference Attack**.\n\n---\n\n## üìñ What is a Membership Inference Attack?\n\nIt tries to determine whether a specific data point was part of a model's training dataset.\n\n**Why does this work?** Models tend to behave slightly differently on data they trained on versus data they have never seen. They are more confident and make fewer errors on training data. An attacker exploits this difference.\n\n**Real world examples:**\n- Determining if a specific person's medical record was in a clinical trial dataset\n- Confirming if someone's financial data trained a credit scoring model\n- Violating GDPR or HIPAA by inferring membership in sensitive datasets\n\n---\n\n## üóÇÔ∏è What We Will Do in This Lab\n\n1. Load the Nursery dataset and understand what it contains\n2. Train a classifier and observe the train vs test accuracy gap\n3. Run a rule-based membership inference attack\n4. Run ART's black-box membership inference attack\n5. Understand why overfitting makes models leak privacy\n\n---\n\n## ‚öôÔ∏è Step 1: Import the Tools We Need"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom art.estimators.classification import SklearnClassifier\nfrom art.attacks.inference.membership_inference import (\n    MembershipInferenceBlackBox,\n    MembershipInferenceBlackBoxRuleBased\n)\n\nnp.random.seed(42)\nprint(\"All tools imported successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üìÇ Step 2: Load and Understand the Nursery Dataset\n\nThe **Nursery dataset** was created to rank applications for nursery school enrollment. It contains information about families that people would consider private ‚Äî financial standing, family structure, housing conditions.\n\nThink of each row as a person's application record. The attack we are about to perform could reveal whether a specific family's private application was used to train the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LOAD THE NURSERY DATASET\n# =============================================================================\n\ncolumn_names = [\n    'parents',   # parents occupation: usual, pretentious, great_pret\n    'has_nurs',  # child nursery quality: proper, less_proper, improper, critical, very_crit\n    'form',      # family form: complete, completed, incomplete, foster\n    'children',  # number of children: 1, 2, 3, more\n    'housing',   # housing conditions: convenient, less_conv, critical\n    'finance',   # financial standing: convenient, inconv\n    'social',    # social conditions: nonprob, slightly_prob, problematic\n    'health',    # health conditions: recommended, priority, not_recom\n    'target'     # enrollment decision: recommend, priority, not_recom, very_recom, spec_prior\n]\n\ndf = pd.read_csv(\n    '../datasets/nursery.data',\n    header=None,\n    names=column_names\n)\n\nprint(f\"Dataset loaded: {len(df)} records\")\nprint(\"\")\nprint(\"First 5 records:\")\nprint(\"-\" * 70)\nprint(df.head().to_string())\nprint(\"\")\nprint(\"Enrollment decision distribution:\")\nprint(df['target'].value_counts())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- What kind of information does this dataset contain about families?\n- Would you consider this information sensitive? Why?\n- If this were a medical dataset, what columns might exist instead?\n\n---\n\n## üî¢ Step 3: Prepare the Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONVERT CATEGORICAL TEXT TO NUMBERS\n# =============================================================================\n# LabelEncoder converts each unique text value into a number.\n# Example: parents column: usual=2, pretentious=1, great_pret=0\n# We apply this to every column including the target.\n# =============================================================================\n\ndf_encoded = df.copy()\nfor column in df_encoded.columns:\n    le = LabelEncoder()\n    df_encoded[column] = le.fit_transform(df_encoded[column])\n\nX = df_encoded.drop('target', axis=1).values\ny = df_encoded['target'].values\n\n# Split into training (members) and testing (non-members)\n# We use these terms deliberately: members = in training set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(f\"Input features: {X.shape[1]} columns per record\")\nprint(\"\")\nprint(f\"Training set : {len(X_train)} records  <- these are the MEMBERS\")\nprint(f\"Testing set  : {len(X_test)} records   <- these are the NON-MEMBERS\")\nprint(\"\")\nprint(\"Members    = records the model trained on\")\nprint(\"Non-members = records the model has never seen\")\nprint(\"The attack will try to tell these two groups apart.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üèãÔ∏è Step 4: Train the Target Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAIN THE TARGET MODEL\n# =============================================================================\n# Random Forest = an ensemble of many decision trees that vote together.\n# n_estimators=100 means we build 100 decision trees.\n# =============================================================================\n\nprint(\"Training target model (Random Forest with 100 trees)...\")\nprint(\"(May take 10-20 seconds)\")\nprint(\"\")\n\ntarget_model = RandomForestClassifier(n_estimators=100, random_state=42)\ntarget_model.fit(X_train, y_train)\n\ntrain_accuracy = accuracy_score(y_train, target_model.predict(X_train))\ntest_accuracy = accuracy_score(y_test, target_model.predict(X_test))\n\nprint(\"Training complete!\")\nprint(\"\")\nprint(\"=\" * 50)\nprint(f\"Training accuracy : {train_accuracy*100:.2f}%\")\nprint(f\"Testing accuracy  : {test_accuracy*100:.2f}%\")\nprint(f\"Gap               : {(train_accuracy - test_accuracy)*100:.2f}%\")\nprint(\"=\" * 50)\nprint(\"\")\nprint(\"IMPORTANT: Notice the gap between training and testing accuracy.\")\nprint(\"This gap is exactly what membership inference attacks exploit.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\nThis is a critical observation. The model performs better on training data than on test data. This is called **overfitting** ‚Äî the model has partially memorised its training examples.\n\nThis overfitting is exactly what membership inference attacks exploit. If the model behaves differently on data it has seen versus data it has not, an attacker can use that difference to identify members.\n\nA perfectly generalising model with no gap would be much harder to attack. Why?\n\n---\n\n## üî¥ Step 5: Rule-Based Membership Inference Attack"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RULE-BASED MEMBERSHIP INFERENCE ATTACK\n# =============================================================================\n# Simple rule:\n#   IF model predicts correctly on a sample -> guess it IS a member\n#   IF model predicts incorrectly           -> guess it is NOT a member\n#\n# This works because models tend to be more accurate on training data.\n# =============================================================================\n\nart_model = SklearnClassifier(model=target_model)\nrule_attack = MembershipInferenceBlackBoxRuleBased(art_model)\n\nsample_size = 200\n\nmember_idx = np.random.choice(len(X_train), sample_size, replace=False)\nX_member = X_train[member_idx]\ny_member = y_train[member_idx]\n\nnonmember_idx = np.random.choice(len(X_test), sample_size, replace=False)\nX_nonmember = X_test[nonmember_idx]\ny_nonmember = y_test[nonmember_idx]\n\n# infer() returns 1 if it thinks sample is a member, 0 if not\nmember_inferred = rule_attack.infer(X_member, y_member)\nnonmember_inferred = rule_attack.infer(X_nonmember, y_nonmember)\n\nmember_accuracy = np.mean(member_inferred == 1)\nnonmember_accuracy = np.mean(nonmember_inferred == 0)\noverall_accuracy = (member_accuracy + nonmember_accuracy) / 2\n\nprint(\"Rule-Based Attack Results:\")\nprint(\"=\" * 50)\nprint(f\"Correctly identified members     : {member_accuracy*100:.1f}%\")\nprint(f\"Correctly identified non-members : {nonmember_accuracy*100:.1f}%\")\nprint(f\"Overall attack accuracy          : {overall_accuracy*100:.1f}%\")\nprint(\"\")\nprint(f\"Random guessing baseline         : 50.0%\")\nprint(f\"Advantage over random guessing   : +{(overall_accuracy-0.5)*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- Random guessing gives 50%. Anything above that means the attacker is gaining real information.\n- Even a small advantage above 50% is a **privacy violation** in a sensitive context like medical data.\n- How much better than random guessing did the rule-based attack perform?\n\n---\n\n## üî¥ Step 6: Black-Box Membership Inference Attack"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# BLACK-BOX MEMBERSHIP INFERENCE ATTACK\n# =============================================================================\n# More sophisticated than rule-based. Trains its own small attack model\n# that learns to distinguish members from non-members based on the victim\n# model's output probabilities.\n#\n# Members tend to get higher confidence scores because the model has seen\n# them before. The attack model learns to exploit this pattern.\n# =============================================================================\n\nprint(\"Running black-box membership inference attack...\")\nprint(\"(Trains an attack model - may take 15-30 seconds)\")\nprint(\"\")\n\nbb_attack = MembershipInferenceBlackBox(art_model, attack_model_type='rf')\n\ntrain_split = sample_size // 2\n\nbb_attack.fit(\n    x=X_member[:train_split],\n    y=y_member[:train_split],\n    x_test=X_nonmember[:train_split],\n    y_test=y_nonmember[:train_split]\n)\n\nprint(\"Attack model trained. Evaluating on held-out data...\")\nprint(\"\")\n\nbb_member_inferred = bb_attack.infer(X_member[train_split:], y_member[train_split:])\nbb_nonmember_inferred = bb_attack.infer(X_nonmember[train_split:], y_nonmember[train_split:])\n\nbb_member_acc = np.mean(bb_member_inferred == 1)\nbb_nonmember_acc = np.mean(bb_nonmember_inferred == 0)\nbb_overall = (bb_member_acc + bb_nonmember_acc) / 2\n\nprint(\"Black-Box Attack Results:\")\nprint(\"=\" * 50)\nprint(f\"Correctly identified members     : {bb_member_acc*100:.1f}%\")\nprint(f\"Correctly identified non-members : {bb_nonmember_acc*100:.1f}%\")\nprint(f\"Overall attack accuracy          : {bb_overall*100:.1f}%\")\nprint(\"\")\nprint(f\"Random guessing baseline         : 50.0%\")\nprint(f\"Advantage over random guessing   : +{(bb_overall-0.5)*100:.1f}%\")\nprint(\"\")\nprint(\"Comparison:\")\nprint(f\"  Rule-based attack : {overall_accuracy*100:.1f}%\")\nprint(f\"  Black-box attack  : {bb_overall*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- Compare rule-based vs black-box results. Which performed better?\n- If this were a medical dataset and the attack had 70% accuracy, what is the real-world privacy implication?\n\n### üß™ Try This\n\nGo back to the model training step and change `n_estimators=100` to `n_estimators=10`. Retrain and rerun the attacks.\n\n- Does a model that overfits more become easier or harder to attack?\n- What does this tell you about the relationship between model quality and privacy?\n\n---\n\n## üí≠ Step 7: Reflect"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "reflection = \"\"\"\nLAB 3 - INFERENCE ATTACK REFLECTION\n=====================================\n\nQ1: In plain English, what is a membership inference attack and why\n    is it a privacy violation?\nA1: [TYPE YOUR ANSWER HERE]\n\nQ2: The attack worked because the model had a training/test accuracy gap\n    (overfitting). What does overfitting mean and why does it help attackers?\nA2: [TYPE YOUR ANSWER HERE]\n\nQ3: Even a small advantage above 50% could be a serious problem in a\n    medical context. What could an attacker do with even partial knowledge\n    of who was in a training dataset?\nA3: [TYPE YOUR ANSWER HERE]\n\nQ4: What defensive measures could reduce membership inference risk?\n    (Hint: look up differential privacy)\nA4: [TYPE YOUR ANSWER HERE]\n\nQ5: Which regulation (GDPR, HIPAA, etc.) would apply if an attack\n    revealed that someone's medical data was in a hospital's AI training set?\nA5: [TYPE YOUR ANSWER HERE]\n\"\"\"\n\nwith open('../outputs/Lab3_Reflection.txt', 'w') as f:\n    f.write(reflection)\n\nprint(\"Reflection saved to outputs/Lab3_Reflection.txt\")\nprint(reflection)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ‚úÖ Lab 3 Complete\n\nReturn to [START_HERE.ipynb](START_HERE.ipynb) and open Lab 4 ‚Äî Extraction Attack.\n\n---\n*Built with the Adversarial Robustness Toolbox (ART) ‚Äî https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
  }
 ]
}