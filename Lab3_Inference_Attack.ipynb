{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ Lab 3 â€” Inference Attack (Membership Inference)\n",
    "### Certified AI Penetration Tester â€“ Red Team (CAIPT-RT)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ The Story\n",
    "\n",
    "A hospital has trained a machine learning model to predict which patients are at risk of a certain disease. The model was trained on real patient records â€” sensitive data including age, family background, financial situation, and health history.\n",
    "\n",
    "The hospital never releases the patient records themselves. But they do offer the trained model as a public-facing tool: doctors and researchers can send it a patient profile and get a risk prediction back.\n",
    "\n",
    "You are an attacker. You do not have the patient records. But you have access to the model. Can you figure out **whether a specific person's data was used to train the model?**\n",
    "\n",
    "If you can, you have just violated that person's privacy â€” you know they were a patient at this hospital and their data was part of a sensitive medical study.\n",
    "\n",
    "This is a **Membership Inference Attack**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– What is a Membership Inference Attack?\n",
    "\n",
    "A membership inference attack tries to determine whether a specific data point was part of a model's training dataset.\n",
    "\n",
    "**Why does this work?** Machine learning models tend to behave slightly differently on data they have seen during training versus data they have never seen. Specifically, models tend to be more confident and make fewer errors on their training data. An attacker can exploit this difference.\n",
    "\n",
    "**Two approaches used in this lab:**\n",
    "\n",
    "**Rule-based attack** â€” simple rule: if the model predicts correctly on a sample, assume it was in the training set. If it predicts incorrectly, assume it was not.\n",
    "\n",
    "**Black-box attack** â€” trains a separate \"attack model\" that learns to distinguish between training members and non-members based on the victim model's output probabilities.\n",
    "\n",
    "**Real world examples:**\n",
    "- Determining if a specific person's medical record was in a clinical trial dataset\n",
    "- Confirming if a person's financial data was used to train a credit scoring model\n",
    "- Violating GDPR/HIPAA by inferring membership in sensitive datasets\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—‚ï¸ What We Will Do in This Lab\n",
    "\n",
    "1. Load the Nursery dataset and understand what it contains\n",
    "2. Train a classifier on the dataset\n",
    "3. Run a rule-based membership inference attack\n",
    "4. Run ART's black-box membership inference attack\n",
    "5. Measure how accurately we can identify training members\n",
    "6. Understand why this is a privacy violation\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Step 1: Import the Tools We Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "# New addition for this lab:\n",
    "# LabelEncoder : converts text category labels into numbers\n",
    "#                The Nursery dataset uses text categories like\n",
    "#                'usual', 'pretentious', 'great_pret' which need\n",
    "#                to become numbers before the model can use them\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# From ART we import the membership inference attack modules\n",
    "from art.estimators.classification import SklearnClassifier\n",
    "from art.attacks.inference.membership_inference import (\n",
    "    MembershipInferenceBlackBox,\n",
    "    MembershipInferenceBlackBoxRuleBased\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All tools imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‚ Step 2: Load and Understand the Nursery Dataset\n",
    "\n",
    "The **Nursery dataset** was originally created to rank applications for nursery school enrollment. It contains information about families applying to nurseries and was used to make enrollment decisions.\n",
    "\n",
    "While it is not a medical dataset, it serves as a perfect stand-in for any sensitive decision-making dataset. The attributes describe family situations that people would consider private â€” financial standing, family structure, housing conditions.\n",
    "\n",
    "Think of each row as a person's application record. The attack we are about to perform could reveal whether a specific family's private application data was used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE NURSERY DATASET\n",
    "# =============================================================================\n",
    "# The nursery.data file has no column headers, so we name them ourselves.\n",
    "# These columns describe family characteristics used to rank applications.\n",
    "# =============================================================================\n",
    "\n",
    "# Column names based on the UCI dataset documentation\n",
    "column_names = [\n",
    "    'parents',      # parents' occupation: usual, pretentious, great_pret\n",
    "    'has_nurs',     # child's nursery: proper, less_proper, improper, critical, very_crit\n",
    "    'form',         # family form: complete, completed, incomplete, foster\n",
    "    'children',     # number of children: 1, 2, 3, more\n",
    "    'housing',      # housing conditions: convenient, less_conv, critical\n",
    "    'finance',      # financial standing: convenient, inconv\n",
    "    'social',       # social conditions: nonprob, slightly_prob, problematic\n",
    "    'health',       # health conditions: recommended, priority, not_recom\n",
    "    'target'        # the decision: recommend, priority, not_recom, very_recom, spec_prior\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '../datasets/nursery.data',\n",
    "    header=None,\n",
    "    names=column_names\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} records\")\n",
    "print(\"\")\n",
    "print(\"First 5 records:\")\n",
    "print(\"-\" * 70)\n",
    "print(df.head().to_string())\n",
    "print(\"\")\n",
    "print(\"Enrollment decision distribution:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ‘€ What Do You See?\n",
    "\n",
    "Look at the dataset columns carefully.\n",
    "\n",
    "- What kind of information does this dataset contain about families?\n",
    "- Would you consider this information sensitive? Why or why not?\n",
    "- If this were a medical dataset instead of a nursery application dataset, what columns might exist?\n",
    "- Look at the enrollment decisions â€” what does 'not_recom' vs 'recommend' mean in terms of real-world impact on families?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ Step 3: Prepare the Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONVERT CATEGORICAL TEXT VALUES TO NUMBERS\n",
    "# =============================================================================\n",
    "# All columns in this dataset contain text categories, not numbers.\n",
    "# Machine learning models need numbers.\n",
    "#\n",
    "# LabelEncoder converts each unique text value into a number.\n",
    "# For example:\n",
    "#   'parents' column: usual=2, pretentious=1, great_pret=0\n",
    "#   'finance' column: convenient=0, inconv=1\n",
    "#\n",
    "# We apply this to every column including the target (what we want to predict)\n",
    "# =============================================================================\n",
    "\n",
    "df_encoded = df.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for column in df_encoded.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[column] = le.fit_transform(df_encoded[column])\n",
    "    label_encoders[column] = le  # save encoders in case we need to reverse later\n",
    "\n",
    "# Separate features (input columns) from target (what we predict)\n",
    "X = df_encoded.drop('target', axis=1).values\n",
    "y = df_encoded['target'].values\n",
    "\n",
    "print(\"Text values converted to numbers successfully.\")\n",
    "print(f\"Input features shape: {X.shape}\")\n",
    "print(f\"  ({X.shape[0]} records, {X.shape[1]} features per record)\")\n",
    "print(\"\")\n",
    "print(\"Example: First record as numbers:\")\n",
    "print(f\"  Original : {df.iloc[0].values}\")\n",
    "print(f\"  Encoded  : {df_encoded.iloc[0].values}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "# We keep track of which records are in the training set\n",
    "# because we will use this knowledge to evaluate the attack later\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"Training set: {len(X_train)} records (these are the 'members')\")\n",
    "print(f\"Testing set : {len(X_test)} records (these are the 'non-members')\")\n",
    "print(\"\")\n",
    "print(\"'Members' = records used to train the model\")\n",
    "print(\"'Non-members' = records the model has never seen\")\n",
    "print(\"The attack will try to tell these two groups apart.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ‘€ What Do You See?\n",
    "\n",
    "- The training set records are called **members** â€” these are the people whose data the model learned from.\n",
    "- The test set records are **non-members** â€” the model has never seen these.\n",
    "- Our attack will try to figure out which records are members, using only the model's predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ‹ï¸ Step 4: Train the Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN THE TARGET MODEL (THE ONE WE WILL ATTACK)\n",
    "# =============================================================================\n",
    "# We use a Random Forest classifier this time.\n",
    "# Random Forest is an ensemble of many decision trees that vote together.\n",
    "# It is a powerful and commonly used model in real-world applications.\n",
    "#\n",
    "# n_estimators=100 means we build 100 decision trees\n",
    "# The more trees, the more powerful but also the slower the training\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training the target model (Random Forest)...\")\n",
    "print(\"(Building 100 decision trees - may take 10-20 seconds)\")\n",
    "print(\"\")\n",
    "\n",
    "target_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "target_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, target_model.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, target_model.predict(X_test))\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training accuracy : {train_accuracy*100:.2f}%\")\n",
    "print(f\"Testing accuracy  : {test_accuracy*100:.2f}%\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\")\n",
    "print(\"IMPORTANT: Notice the gap between training and testing accuracy.\")\n",
    "print(\"This gap is key to why membership inference attacks work.\")\n",
    "print(f\"The gap here is: {(train_accuracy - test_accuracy)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ‘€ What Do You See?\n",
    "\n",
    "This is a critical observation. Look at the gap between training accuracy and testing accuracy.\n",
    "\n",
    "- The model performs better on training data than on test data. This is called **overfitting** â€” the model has partially memorized its training examples rather than learning general patterns.\n",
    "- This overfitting is exactly what membership inference attacks exploit. If the model behaves differently on data it has seen versus data it has not seen, an attacker can use that difference to identify members.\n",
    "- A perfectly generalizing model (same accuracy on train and test) would be much harder to attack with membership inference. Why?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”´ Step 5: Attack â€” Rule-Based Membership Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WRAP MODEL IN ART AND RUN RULE-BASED ATTACK\n",
    "# =============================================================================\n",
    "# First we wrap our model in ART's SklearnClassifier.\n",
    "# Then we run the simplest attack: rule-based membership inference.\n",
    "#\n",
    "# Rule-based attack logic:\n",
    "#   IF the model predicts correctly on a sample -> guess it is a MEMBER\n",
    "#   IF the model predicts incorrectly           -> guess it is NOT a member\n",
    "#\n",
    "# This works because models tend to be more accurate on training data.\n",
    "# =============================================================================\n",
    "\n",
    "# Wrap the model\n",
    "art_model = SklearnClassifier(model=target_model)\n",
    "\n",
    "# Create the rule-based attack\n",
    "rule_attack = MembershipInferenceBlackBoxRuleBased(art_model)\n",
    "\n",
    "print(\"Running rule-based membership inference attack...\")\n",
    "print(\"\")\n",
    "\n",
    "# We need to test on both members (training data) and non-members (test data)\n",
    "# to see if the attack can tell them apart\n",
    "#\n",
    "# Use a sample of 200 from each group for a fair comparison\n",
    "sample_size = 200\n",
    "\n",
    "# Sample from training set (members)\n",
    "member_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_member = X_train[member_idx]\n",
    "y_member = y_train[member_idx]\n",
    "\n",
    "# Sample from test set (non-members)\n",
    "nonmember_idx = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "X_nonmember = X_test[nonmember_idx]\n",
    "y_nonmember = y_test[nonmember_idx]\n",
    "\n",
    "# Run the attack\n",
    "# infer() returns 1 if it thinks the sample is a member, 0 if not\n",
    "member_inferred = rule_attack.infer(X_member, y_member)\n",
    "nonmember_inferred = rule_attack.infer(X_nonmember, y_nonmember)\n",
    "\n",
    "# Calculate attack accuracy\n",
    "# For members: attack is correct if it predicts 1 (is a member)\n",
    "member_accuracy = np.mean(member_inferred == 1)\n",
    "# For non-members: attack is correct if it predicts 0 (not a member)\n",
    "nonmember_accuracy = np.mean(nonmember_inferred == 0)\n",
    "# Overall attack accuracy\n",
    "overall_accuracy = (member_accuracy + nonmember_accuracy) / 2\n",
    "\n",
    "print(\"Rule-Based Attack Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Correctly identified members     : {member_accuracy*100:.1f}%\")\n",
    "print(f\"Correctly identified non-members : {nonmember_accuracy*100:.1f}%\")\n",
    "print(f\"Overall attack accuracy          : {overall_accuracy*100:.1f}%\")\n",
    "print(\"\")\n",
    "print(f\"Random guessing baseline         : 50.0%\")\n",
    "print(f\"Advantage over random guessing   : +{(overall_accuracy-0.5)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ‘€ What Do You See?\n",
    "\n",
    "- The random guessing baseline is 50% â€” that is what you would get if you just guessed randomly.\n",
    "- If the attack accuracy is above 50%, the attacker is gaining real information about who was in the training set.\n",
    "- Even a small advantage above 50% is a **privacy violation** in a sensitive context like medical data.\n",
    "- How much better than random guessing did the rule-based attack do?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”´ Step 6: Attack â€” Black-Box Membership Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BLACK-BOX MEMBERSHIP INFERENCE ATTACK\n",
    "# =============================================================================\n",
    "# The black-box attack is more sophisticated than the rule-based approach.\n",
    "# Instead of using a simple rule, it trains its own small \"attack model\"\n",
    "# that learns to distinguish members from non-members.\n",
    "#\n",
    "# How it works:\n",
    "#   1. The attack model looks at the TARGET model's output probabilities\n",
    "#      (how confident the model is about each class for each sample)\n",
    "#   2. It learns that members tend to get higher confidence scores\n",
    "#      because the model has seen them before\n",
    "#   3. It uses this pattern to predict membership\n",
    "#\n",
    "# attack_model_type='rf' means we use a Random Forest as the attack model\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Running black-box membership inference attack...\")\n",
    "print(\"(This trains an attack model - may take 15-30 seconds)\")\n",
    "print(\"\")\n",
    "\n",
    "# Create the black-box attack\n",
    "bb_attack = MembershipInferenceBlackBox(art_model, attack_model_type='rf')\n",
    "\n",
    "# The attack needs some data to train its own internal attack model.\n",
    "# We give it half of our members and half of our non-members to learn from.\n",
    "# The other half is used to evaluate the attack.\n",
    "train_split = sample_size // 2\n",
    "\n",
    "# Train the attack model\n",
    "bb_attack.fit(\n",
    "    x=X_member[:train_split],           # known members for training\n",
    "    y=y_member[:train_split],\n",
    "    x_test=X_nonmember[:train_split],   # known non-members for training\n",
    "    y_test=y_nonmember[:train_split]\n",
    ")\n",
    "\n",
    "print(\"Attack model trained. Now evaluating on held-out data...\")\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate on the remaining half (data the attack model has not seen)\n",
    "bb_member_inferred = bb_attack.infer(\n",
    "    X_member[train_split:],\n",
    "    y_member[train_split:]\n",
    ")\n",
    "bb_nonmember_inferred = bb_attack.infer(\n",
    "    X_nonmember[train_split:],\n",
    "    y_nonmember[train_split:]\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "bb_member_acc = np.mean(bb_member_inferred == 1)\n",
    "bb_nonmember_acc = np.mean(bb_nonmember_inferred == 0)\n",
    "bb_overall = (bb_member_acc + bb_nonmember_acc) / 2\n",
    "\n",
    "print(\"Black-Box Attack Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Correctly identified members     : {bb_member_acc*100:.1f}%\")\n",
    "print(f\"Correctly identified non-members : {bb_nonmember_acc*100:.1f}%\")\n",
    "print(f\"Overall attack accuracy          : {bb_overall*100:.1f}%\")\n",
    "print(\"\")\n",
    "print(f\"Random guessing baseline         : 50.0%\")\n",
    "print(f\"Advantage over random guessing   : +{(bb_overall-0.5)*100:.1f}%\")\n",
    "print(\"\")\n",
    "print(\"Comparison:\")\n",
    "print(f\"  Rule-based attack : {overall_accuracy*100:.1f}%\")\n",
    "print(f\"  Black-box attack  : {bb_overall*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ‘€ What Do You See?\n",
    "\n",
    "- Compare the rule-based and black-box attack results. Which performed better?\n",
    "- The black-box attack uses a smarter approach â€” does that always mean it performs better? Why or why not?\n",
    "- If this were a medical dataset and the attack had 70% accuracy at identifying members, what is the real-world privacy implication?\n",
    "\n",
    "### ðŸ§ª Try This\n",
    "\n",
    "Go back to where we trained the target model and change `n_estimators=100` to `n_estimators=10` (a weaker model that overfits more). Retrain and rerun the attacks.\n",
    "\n",
    "- Does a model that overfits more become easier or harder to attack with membership inference?\n",
    "- What does this tell you about the relationship between model quality and privacy?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’­ Step 7: Reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REFLECTION - SAVE YOUR ANSWERS\n",
    "# =============================================================================\n",
    "\n",
    "reflection = \"\"\"\n",
    "LAB 3 - INFERENCE ATTACK REFLECTION\n",
    "=====================================\n",
    "\n",
    "Q1: In plain English, what is a membership inference attack?\n",
    "    What does the attacker learn, and why is that a privacy violation?\n",
    "A1: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q2: The attack worked because the model had a gap between training\n",
    "    accuracy and test accuracy (overfitting). What does overfitting mean\n",
    "    in plain English, and why does it make membership inference easier?\n",
    "A2: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q3: The attack accuracy was above 50% (random guessing baseline).\n",
    "    Even a small advantage (say, 55%) could be a serious problem.\n",
    "    In a medical context, what could an attacker do with even partial\n",
    "    knowledge of who was in a training dataset?\n",
    "A3: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q4: What defensive measures could reduce the risk of membership\n",
    "    inference attacks? Think about: model training techniques,\n",
    "    output restrictions, and data handling.\n",
    "    (Hint: look up 'differential privacy' as a starting point)\n",
    "A4: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q5: Which regulation (GDPR, HIPAA, etc.) would be most relevant\n",
    "    if a membership inference attack revealed that a specific person's\n",
    "    medical data was in a hospital's AI training set?\n",
    "A5: [TYPE YOUR ANSWER HERE]\n",
    "\"\"\"\n",
    "\n",
    "with open('../outputs/Lab3_Reflection.txt', 'w') as f:\n",
    "    f.write(reflection)\n",
    "\n",
    "print(\"Reflection saved to outputs/Lab3_Reflection.txt\")\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Lab 3 Complete\n",
    "\n",
    "You have successfully:\n",
    "- Loaded and understood a sensitive categorical dataset\n",
    "- Trained a Random Forest classifier and observed the train/test accuracy gap\n",
    "- Run a rule-based membership inference attack\n",
    "- Run ART's black-box membership inference attack\n",
    "- Understood why overfitting makes models vulnerable to privacy attacks\n",
    "\n",
    "When you are ready, return to [START_HERE.ipynb](START_HERE.ipynb) and open Lab 4 â€” Extraction Attack.\n",
    "\n",
    "---\n",
    "*Lab built with the Adversarial Robustness Toolbox (ART)*  \n",
    "*https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
   ]
  }
 ],
 "metadata": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.12.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
