{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üî¥ Lab 2 ‚Äî Poisoning Attack\n### Certified AI Penetration Tester ‚Äì Red Team (CAIPT-RT)\n\n---\n\n## üéØ The Story\n\nA company is building a spam filter. The training dataset is stored in a shared folder that multiple people can access during data collection. You are an attacker who has quietly slipped corrupted examples into that folder before training begins. When the model trains on your poisoned data, it learns the wrong lessons ‚Äî and you have permanently damaged it without ever touching the model's code.\n\nThis is a **Poisoning Attack**. You corrupt the data the model learns from.\n\n---\n\n## üìñ What is a Poisoning Attack?\n\nA poisoning attack targets the **training phase** ‚Äî before the model learns. The attacker injects bad examples into the training dataset.\n\n**Two main types:**\n- **Label poisoning** ‚Äî changing labels of real examples (relabeling spam as legitimate)\n- **Data poisoning** ‚Äî injecting entirely fake examples to push the model in a harmful direction\n\n**Real world examples:**\n- Corrupting training data for a fraud detection model so it misses certain patterns\n- Poisoning a medical diagnosis model to misclassify certain conditions\n- Poisoning a content moderation model to allow harmful content through\n\n---\n\n## üóÇÔ∏è What We Will Do in This Lab\n\n1. Load the SMS spam dataset and train a clean baseline model\n2. Record the clean model's accuracy as our benchmark\n3. Inject poisoned examples at different rates\n4. Retrain on poisoned data and compare accuracy\n5. Visualise the damage\n\n---\n\n## ‚öôÔ∏è Step 1: Import the Tools We Need"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport copy\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nnp.random.seed(42)\nprint(\"All tools imported successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üìÇ Step 2: Load Dataset and Train a Clean Baseline Model\n\nBefore measuring damage from an attack, we need to know how well the model performs **without** any attack. This is our **baseline** ‚Äî the reference point everything else is measured against."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LOAD DATASET\n# =============================================================================\n\ndf = pd.read_csv(\n    '../datasets/SMSSpamCollection',\n    sep='\\t',\n    header=None,\n    names=['label', 'message'],\n    encoding='latin-1'\n)\n\ndf['label_num'] = df['label'].map({'spam': 1, 'ham': 0})\n\nprint(f\"Dataset loaded: {len(df)} messages\")\nprint(f\"Spam: {sum(df.label_num==1)} | Ham: {sum(df.label_num==0)}\")\nprint(\"\")\n\n# Convert text to numbers\nvectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(df['message']).toarray()\ny = df['label_num'].values\n\n# Split - test set is kept clean and never touched\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set : {len(X_train)} messages\")\nprint(f\"Testing set  : {len(X_test)} messages (kept clean throughout)\")\nprint(\"\")\n\n# Train the clean baseline model\nprint(\"Training clean baseline model...\")\nclean_model = LogisticRegression(max_iter=1000, random_state=42)\nclean_model.fit(X_train, y_train)\n\nclean_predictions = clean_model.predict(X_test)\nclean_accuracy = accuracy_score(y_test, clean_predictions)\n\nprint(\"\")\nprint(\"=\" * 50)\nprint(f\"BASELINE (Clean Model) Accuracy: {clean_accuracy*100:.2f}%\")\nprint(\"=\" * 50)\nprint(\"\")\nprint(\"Remember this number. Any drop after poisoning is attack damage.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- Write down the clean model's accuracy. This is your benchmark.\n- After the poisoning attack, any drop below this number represents damage caused by the attacker.\n\n---\n\n## ‚ò†Ô∏è Step 3: Label Flipping ‚Äî The Poisoning Attack\n\nWe perform **label flipping**: take real spam messages and relabel them as legitimate. When the model trains on this corrupted data, it learns that these spam messages are acceptable.\n\nWe run the attack at three different poisoning rates to see how damage scales."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LABEL FLIPPING POISONING ATTACK\n# =============================================================================\n\ndef label_flip_attack(X_train, y_train, poison_rate):\n    \"\"\"\n    Flips labels of a percentage of spam messages to ham.\n\n    Parameters:\n        X_train     : training message vectors\n        y_train     : original correct labels\n        poison_rate : fraction of spam to mislabel (e.g. 0.1 = 10%)\n\n    Returns:\n        X_train     : unchanged (we only flip labels, not data)\n        y_poisoned  : labels with some spam relabeled as ham\n        n_poisoned  : how many labels were flipped\n    \"\"\"\n    y_poisoned = copy.deepcopy(y_train)\n    spam_indices = np.where(y_train == 1)[0]\n    n_to_poison = int(len(spam_indices) * poison_rate)\n    poison_indices = np.random.choice(spam_indices, n_to_poison, replace=False)\n    y_poisoned[poison_indices] = 0  # flip spam to ham\n    return X_train, y_poisoned, n_to_poison\n\n\npoison_rates = [0.05, 0.10, 0.20]\nresults = []\n\nprint(\"Running label flipping attack at different poisoning rates...\")\nprint(\"=\" * 65)\nprint(f\"{'Poison Rate':<15} {'Messages Flipped':<20} {'Accuracy':<15} {'Drop'}\")\nprint(\"-\" * 65)\n\nfor rate in poison_rates:\n    X_p, y_p, n_poisoned = label_flip_attack(X_train, y_train, rate)\n\n    poisoned_model = LogisticRegression(max_iter=1000, random_state=42)\n    poisoned_model.fit(X_p, y_p)\n\n    # Always test on the CLEAN test set for an honest measurement\n    poisoned_preds = poisoned_model.predict(X_test)\n    poisoned_accuracy = accuracy_score(y_test, poisoned_preds)\n    drop = clean_accuracy - poisoned_accuracy\n    results.append((rate, n_poisoned, poisoned_accuracy, drop, poisoned_model))\n\n    print(f\"{rate*100:.0f}%{'':<12} {n_poisoned:<20} {poisoned_accuracy*100:.2f}%{'':<9} -{drop*100:.2f}%\")\n\nprint(\"-\" * 65)\nprint(f\"Baseline (no attack):{'':<18} {clean_accuracy*100:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- How does accuracy change as more labels are flipped?\n- Even 5% poisoning causes a measurable drop. What does this tell you about how sensitive models are to data quality?\n- At 20% poisoning, how many more spam messages reach users compared to the clean model?\n\n---\n\n## üìä Step 4: Visualise the Damage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# VISUALISE POISONING IMPACT\n# =============================================================================\n\nrates = [r[0]*100 for r in results]\naccuracies = [r[2]*100 for r in results]\n\nplt.figure(figsize=(8, 5))\nplt.axhline(\n    y=clean_accuracy*100,\n    color='green', linestyle='--',\n    label=f'Clean baseline ({clean_accuracy*100:.2f}%)'\n)\nplt.plot(rates, accuracies, 'ro-', linewidth=2, markersize=8, label='Poisoned model')\nfor rate, acc in zip(rates, accuracies):\n    plt.annotate(f'{acc:.2f}%', (rate, acc), textcoords=\"offset points\", xytext=(0, 10))\n\nplt.title('Impact of Label Flipping Poisoning Attack on Model Accuracy')\nplt.xlabel('Poisoning Rate (% of spam labels flipped)')\nplt.ylabel('Model Accuracy (%)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('../outputs/lab2_poisoning_impact.png')\nplt.show()\nprint(\"Chart saved to outputs folder.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- The green dashed line is where the model should be. The red line shows where it actually performs after poisoning.\n- Is the relationship between poisoning rate and accuracy drop linear, or does it accelerate?\n- At what poisoning rate would you consider the filter completely broken?\n\n### üß™ Try This\n\nGo back to the attack and try `poison_rate=0.50`. At 50% poisoning, is the spam filter still doing better than random guessing?\n\n---\n\n## üí≠ Step 5: Reflect"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "reflection = \"\"\"\nLAB 2 - POISONING ATTACK REFLECTION\n=====================================\n\nQ1: In plain English, what is a poisoning attack and when does it happen?\nA1: [TYPE YOUR ANSWER HERE]\n\nQ2: Describe what label flipping does and why it damages the model.\nA2: [TYPE YOUR ANSWER HERE]\n\nQ3: Who has access to training data before a model is trained in a real\n    organisation? What access controls would you recommend?\nA3: [TYPE YOUR ANSWER HERE]\n\nQ4: Compare evasion (Lab 1) and poisoning (Lab 2). Which is harder to\n    detect? Which causes more lasting damage?\nA4: [TYPE YOUR ANSWER HERE]\n\nQ5: Name a real-world AI system where a poisoning attack could have\n    serious consequences.\nA5: [TYPE YOUR ANSWER HERE]\n\"\"\"\n\nwith open('../outputs/Lab2_Reflection.txt', 'w') as f:\n    f.write(reflection)\n\nprint(\"Reflection saved to outputs/Lab2_Reflection.txt\")\nprint(reflection)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ‚úÖ Lab 2 Complete\n\nReturn to [START_HERE.ipynb](START_HERE.ipynb) and open Lab 3 ‚Äî Inference Attack.\n\n---\n*Built with the Adversarial Robustness Toolbox (ART) ‚Äî https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
  }
 ]
}