{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ Lab 2 ‚Äî Poisoning Attack\n",
    "### Certified AI Penetration Tester ‚Äì Red Team (CAIPT-RT)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Story\n",
    "\n",
    "Imagine you work at a company that is building a spam filter. The filter is trained on a large dataset of labeled messages. That dataset is stored in a shared folder that multiple people have access to during the data collection phase.\n",
    "\n",
    "You are an attacker who has managed to get access to that shared folder **before training begins**. You do not need to touch the model itself ‚Äî instead, you quietly slip corrupted examples into the training data. When the model trains on your poisoned data, it learns the wrong lessons ‚Äî and you have permanently damaged it without ever touching the model's code.\n",
    "\n",
    "This is a **Poisoning Attack**. You corrupt the data the model learns from.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ What is a Poisoning Attack?\n",
    "\n",
    "A poisoning attack targets the **training phase** of a machine learning model ‚Äî before or during the time the model is learning. The attacker injects carefully crafted bad examples into the training dataset.\n",
    "\n",
    "There are two main types:\n",
    "\n",
    "**Label poisoning** ‚Äî the attacker changes the labels of real examples. For instance, relabeling spam messages as legitimate so the model learns that spam is acceptable.\n",
    "\n",
    "**Data poisoning** ‚Äî the attacker injects entirely fake examples designed to push the model's decision boundary in a harmful direction.\n",
    "\n",
    "**Real world examples:**\n",
    "- Corrupting training data for a fraud detection model so it misses certain fraud patterns\n",
    "- Poisoning a medical diagnosis model to misclassify certain conditions\n",
    "- Poisoning a content moderation model to allow harmful content through\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è What We Will Do in This Lab\n",
    "\n",
    "1. Load the SMS spam dataset and train a clean baseline model\n",
    "2. Record the clean model's accuracy ‚Äî this is our benchmark\n",
    "3. Inject poisoned examples into the training data\n",
    "4. Retrain the model on the poisoned data\n",
    "5. Compare accuracy before and after poisoning\n",
    "6. Experiment with different poisoning rates\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 1: Import the Tools We Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "# Same libraries as Lab 1 with one addition:\n",
    "# copy : allows us to make exact copies of data without modifying the original\n",
    "#        This is important because we want to keep the clean data safe\n",
    "#        while we create a poisoned version to compare against\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# From ART we use the poisoning attack\n",
    "# PoisoningAttackSVM is a gradient-based poisoning attack\n",
    "# We will also demonstrate a simpler label-flipping approach\n",
    "# to make the concept clear before using ART's more advanced attack\n",
    "from art.estimators.classification import SklearnClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All tools imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÇ Step 2: Load the Dataset and Train a Clean Baseline Model\n",
    "\n",
    "Before we can measure the damage from a poisoning attack, we need to know how well the model performs **without** any attack. This is called the **baseline** ‚Äî our reference point.\n",
    "\n",
    "We will train a clean model first, record its accuracy, then poison the data and retrain. The difference in accuracy tells us how damaging the attack was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATASET\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '../datasets/SMSSpamCollection',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['label', 'message'],\n",
    "    encoding='latin-1'\n",
    ")\n",
    "\n",
    "# Convert labels to numbers: spam=1, ham=0\n",
    "df['label_num'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} messages\")\n",
    "print(f\"Spam: {sum(df.label_num==1)} | Ham: {sum(df.label_num==0)}\")\n",
    "print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONVERT TEXT TO NUMBERS\n",
    "# =============================================================================\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['message']).toarray()\n",
    "y = df['label_num'].values\n",
    "\n",
    "# =============================================================================\n",
    "# SPLIT INTO TRAINING AND TESTING SETS\n",
    "# =============================================================================\n",
    "# We keep the test set completely separate and never touch it.\n",
    "# The test set is used only for measuring accuracy - never for training.\n",
    "# This ensures our accuracy measurements are fair and honest.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} messages\")\n",
    "print(f\"Testing set : {len(X_test)} messages\")\n",
    "print(\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN THE CLEAN BASELINE MODEL\n",
    "# =============================================================================\n",
    "# This model trains on completely clean, unmodified data.\n",
    "# Its accuracy becomes our benchmark - the score we expect a healthy model\n",
    "# to achieve. Any drop in accuracy after poisoning tells us the attack worked.\n",
    "\n",
    "print(\"Training clean baseline model...\")\n",
    "clean_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clean_model.fit(X_train, y_train)\n",
    "\n",
    "# Measure baseline accuracy on the test set\n",
    "clean_predictions = clean_model.predict(X_test)\n",
    "clean_accuracy = accuracy_score(y_test, clean_predictions)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"BASELINE (Clean Model) Accuracy: {clean_accuracy*100:.2f}%\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\")\n",
    "print(\"This is our benchmark. Remember this number.\")\n",
    "print(\"After poisoning, we will compare against it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "- What is the clean model's accuracy? Write this number down ‚Äî it is your baseline.\n",
    "- This is how the spam filter performs when everything is working correctly.\n",
    "- After the poisoning attack, any drop below this number is damage caused by the attack.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ò†Ô∏è Step 3: Understand Label Flipping ‚Äî The Simplest Poisoning Attack\n",
    "\n",
    "Before using ART's advanced attack, we will first demonstrate the simplest form of poisoning: **label flipping**.\n",
    "\n",
    "Label flipping means an attacker takes real spam messages and relabels them as legitimate. When the model trains on this corrupted data, it learns that these spam messages are acceptable ‚Äî and will let similar messages through in the future.\n",
    "\n",
    "This is the most intuitive poisoning attack and helps build understanding before we move to more sophisticated methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LABEL FLIPPING POISONING ATTACK\n",
    "# =============================================================================\n",
    "# We will run this attack at three different poisoning rates:\n",
    "#   5%  - attacker flips 5% of spam labels to ham\n",
    "#   10% - attacker flips 10% of spam labels to ham  \n",
    "#   20% - attacker flips 20% of spam labels to ham\n",
    "#\n",
    "# This lets us see how the damage scales with the amount of poisoning.\n",
    "# =============================================================================\n",
    "\n",
    "def label_flip_attack(X_train, y_train, poison_rate):\n",
    "    \"\"\"\n",
    "    Performs a label flipping poisoning attack.\n",
    "    \n",
    "    Takes the training data and flips the labels of a percentage of\n",
    "    spam messages to make them look like legitimate messages.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train     : the training message vectors\n",
    "        y_train     : the original correct labels\n",
    "        poison_rate : fraction of spam messages to mislabel (e.g. 0.1 = 10%)\n",
    "    \n",
    "    Returns:\n",
    "        X_poisoned  : training data with poisoned samples added\n",
    "        y_poisoned  : labels with some spam relabeled as ham\n",
    "        n_poisoned  : how many labels were flipped\n",
    "    \"\"\"\n",
    "    # Make a copy of the labels so we do not modify the original\n",
    "    y_poisoned = copy.deepcopy(y_train)\n",
    "    \n",
    "    # Find all spam messages in the training set\n",
    "    spam_indices = np.where(y_train == 1)[0]\n",
    "    \n",
    "    # Calculate how many to poison based on the rate\n",
    "    n_to_poison = int(len(spam_indices) * poison_rate)\n",
    "    \n",
    "    # Randomly select which spam messages to mislabel\n",
    "    poison_indices = np.random.choice(spam_indices, n_to_poison, replace=False)\n",
    "    \n",
    "    # Flip their labels from spam (1) to ham (0)\n",
    "    y_poisoned[poison_indices] = 0\n",
    "    \n",
    "    return X_train, y_poisoned, n_to_poison\n",
    "\n",
    "\n",
    "# Run the attack at three different poison rates\n",
    "poison_rates = [0.05, 0.10, 0.20]\n",
    "results = []\n",
    "\n",
    "print(\"Running label flipping attack at different poisoning rates...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Poison Rate':<15} {'Messages Flipped':<20} {'Model Accuracy':<15} {'Accuracy Drop'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for rate in poison_rates:\n",
    "    # Create poisoned training data\n",
    "    X_p, y_p, n_poisoned = label_flip_attack(X_train, y_train, rate)\n",
    "    \n",
    "    # Train a new model on the poisoned data\n",
    "    poisoned_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    poisoned_model.fit(X_p, y_p)\n",
    "    \n",
    "    # Test the poisoned model on the CLEAN test set\n",
    "    # (we always test on clean data to see the true impact)\n",
    "    poisoned_preds = poisoned_model.predict(X_test)\n",
    "    poisoned_accuracy = accuracy_score(y_test, poisoned_preds)\n",
    "    \n",
    "    drop = clean_accuracy - poisoned_accuracy\n",
    "    results.append((rate, n_poisoned, poisoned_accuracy, drop))\n",
    "    \n",
    "    print(f\"{rate*100:.0f}%{'':<12} {n_poisoned:<20} {poisoned_accuracy*100:.2f}%{'':<9} -{drop*100:.2f}%\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Baseline (no attack):                        {clean_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "Look at the table above carefully.\n",
    "\n",
    "- How does the model's accuracy change as more labels are flipped?\n",
    "- Even a small amount of poisoning (5%) causes a measurable drop. What does this tell you about how sensitive machine learning models are to data quality?\n",
    "- At 20% poisoning, how many more spam messages would get through to users compared to the clean model?\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Step 4: Visualize the Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE THE POISONING IMPACT\n",
    "# =============================================================================\n",
    "# A chart makes the relationship between poisoning rate and accuracy drop\n",
    "# much easier to understand and present to others.\n",
    "# =============================================================================\n",
    "\n",
    "rates = [r[0]*100 for r in results]\n",
    "accuracies = [r[2]*100 for r in results]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the clean baseline as a horizontal reference line\n",
    "plt.axhline(\n",
    "    y=clean_accuracy*100,\n",
    "    color='green',\n",
    "    linestyle='--',\n",
    "    label=f'Clean baseline ({clean_accuracy*100:.2f}%)'\n",
    ")\n",
    "\n",
    "# Plot the poisoned model accuracies\n",
    "plt.plot(rates, accuracies, 'ro-', linewidth=2, markersize=8, label='Poisoned model')\n",
    "\n",
    "# Add value labels on each point\n",
    "for rate, acc in zip(rates, accuracies):\n",
    "    plt.annotate(f'{acc:.2f}%', (rate, acc), textcoords=\"offset points\", xytext=(0, 10))\n",
    "\n",
    "plt.title('Impact of Label Flipping Poisoning Attack on Spam Filter Accuracy')\n",
    "plt.xlabel('Poisoning Rate (% of spam labels flipped)')\n",
    "plt.ylabel('Model Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/lab2_poisoning_impact.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved to outputs folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "- The green dashed line is where the model should be performing. The red line shows where it actually performs after poisoning.\n",
    "- Is the relationship between poisoning rate and accuracy drop linear (a straight line) or does it accelerate?\n",
    "- If you were operating a spam filter at a large company, at what poisoning rate would you consider the filter completely broken?\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Step 5: Look at What the Poisoned Model Gets Wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMINE WHAT THE POISONED MODEL GETS WRONG\n",
    "# =============================================================================\n",
    "# It is not enough to know accuracy dropped. We need to understand HOW\n",
    "# the model fails. Does it now:\n",
    "#   a) Miss more spam (false negatives) - spam gets through to users\n",
    "#   b) Flag more legitimate messages (false positives) - legit msgs blocked\n",
    "#\n",
    "# For a spam filter, false negatives are usually more dangerous\n",
    "# (spam gets through) than false positives (legitimate mail gets blocked).\n",
    "# The poisoning attack is specifically designed to cause false negatives.\n",
    "# =============================================================================\n",
    "\n",
    "# Use the most aggressive poisoning (20%) for this analysis\n",
    "X_p20, y_p20, _ = label_flip_attack(X_train, y_train, 0.20)\n",
    "worst_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "worst_model.fit(X_p20, y_p20)\n",
    "worst_preds = worst_model.predict(X_test)\n",
    "\n",
    "print(\"Comparing Clean Model vs 20% Poisoned Model:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "print(\"CLEAN MODEL performance:\")\n",
    "print(classification_report(y_test, clean_predictions, target_names=['Ham', 'Spam']))\n",
    "print(\"\")\n",
    "print(\"POISONED MODEL (20% label flip) performance:\")\n",
    "print(classification_report(y_test, worst_preds, target_names=['Ham', 'Spam']))\n",
    "\n",
    "# Count specific failure types\n",
    "spam_test_indices = np.where(y_test == 1)[0]\n",
    "clean_missed = sum(clean_predictions[spam_test_indices] == 0)\n",
    "poisoned_missed = sum(worst_preds[spam_test_indices] == 0)\n",
    "\n",
    "print(f\"Spam messages missed by clean model    : {clean_missed}\")\n",
    "print(f\"Spam messages missed by poisoned model : {poisoned_missed}\")\n",
    "print(f\"Extra spam getting through after attack: {poisoned_missed - clean_missed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "- Compare the recall score for spam between the clean and poisoned models. Recall for spam means \"out of all actual spam, how much did the model catch?\" A lower recall means more spam is getting through.\n",
    "- How many additional spam messages get through after the poisoning attack?\n",
    "- Did the poisoning also affect the model's ability to handle legitimate messages, or was the damage targeted specifically at spam detection?\n",
    "\n",
    "### üß™ Try This\n",
    "\n",
    "Go back to the label_flip_attack function call and try `poison_rate=0.50` ‚Äî poisoning half of all spam labels. Run the comparison again.\n",
    "\n",
    "- At 50% poisoning, is the spam filter still doing better than random guessing?\n",
    "- What does this tell you about the upper limit of how bad a poisoning attack can get?\n",
    "\n",
    "---\n",
    "\n",
    "## üí≠ Step 6: Reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REFLECTION - SAVE YOUR ANSWERS\n",
    "# =============================================================================\n",
    "\n",
    "reflection = \"\"\"\n",
    "LAB 2 - POISONING ATTACK REFLECTION\n",
    "=====================================\n",
    "\n",
    "Q1: In plain English, what is a poisoning attack and when does it happen\n",
    "    in the machine learning pipeline?\n",
    "A1: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q2: In this lab we used label flipping. Describe in your own words what\n",
    "    label flipping does and why it damages the model.\n",
    "A2: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q3: You saw that even 5% poisoning caused a measurable accuracy drop.\n",
    "    In a real organization, who has access to training data before a model\n",
    "    is trained? What access controls would you recommend to prevent poisoning?\n",
    "A3: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q4: Compare evasion attacks (Lab 1) and poisoning attacks (Lab 2).\n",
    "    Which do you think is harder to detect? Which causes more lasting damage?\n",
    "A4: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q5: Name a real-world AI system where a poisoning attack could have\n",
    "    serious consequences. Describe the attack and its impact.\n",
    "A5: [TYPE YOUR ANSWER HERE]\n",
    "\"\"\"\n",
    "\n",
    "with open('../outputs/Lab2_Reflection.txt', 'w') as f:\n",
    "    f.write(reflection)\n",
    "\n",
    "print(\"Reflection saved to outputs/Lab2_Reflection.txt\")\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Lab 2 Complete\n",
    "\n",
    "You have successfully:\n",
    "- Trained a clean baseline spam filter and recorded its accuracy\n",
    "- Performed a label flipping poisoning attack at multiple rates\n",
    "- Measured and visualized the accuracy damage caused by poisoning\n",
    "- Identified exactly which type of errors the poisoned model makes\n",
    "\n",
    "When you are ready, return to [START_HERE.ipynb](START_HERE.ipynb) and open Lab 3 ‚Äî Inference Attack.\n",
    "\n",
    "---\n",
    "*Lab built with the Adversarial Robustness Toolbox (ART)*  \n",
    "*https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
