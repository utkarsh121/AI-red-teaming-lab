{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üî¥ Lab 4 ‚Äî Extraction Attack (Model Stealing)\n### Certified AI Penetration Tester ‚Äì Red Team (CAIPT-RT)\n\n---\n\n## üéØ The Story\n\nA company spent years building a machine learning model that predicts whether families qualify for social services. The model is their competitive advantage. The code, weights, and training data are all kept secret.\n\nBut they offer it as an API service. You send it an application, it sends back a decision. That is all you get.\n\nYou are an attacker ‚Äî perhaps a competitor, perhaps a researcher exposing bias. You have no access to the model internals. But you have access to the API.\n\nBy sending thousands of carefully chosen queries and recording the responses, you can **build your own model that behaves almost identically** to the original.\n\nThis is a **Model Extraction Attack** ‚Äî also called model stealing.\n\n---\n\n## üìñ What is a Model Extraction Attack?\n\nThe attacker creates a functional copy of a model by repeatedly querying it and using the query-response pairs as training data for a new model.\n\n**Why is this a problem?**\n- Stolen intellectual property ‚Äî years of R&D reproduced for free\n- The stolen model can be used to prepare better evasion attacks locally\n- Researchers sometimes use it to expose bias in proprietary models\n\n**Real world examples:**\n- Stealing a competitor's fraud detection model\n- Copying a medical diagnosis model to avoid licensing fees\n- Using a stolen model as a stepping stone for further attacks\n\n---\n\n## üóÇÔ∏è What We Will Do in This Lab\n\n1. Train the victim model ‚Äî the valuable model being stolen\n2. Set up a black-box query interface simulating API access\n3. Use ART's extraction attack to steal the model\n4. Compare stolen model quality at different query volumes\n5. Think like a defender\n\n---\n\n## ‚öôÔ∏è Step 1: Import the Tools We Need"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom art.estimators.classification import SklearnClassifier\nfrom art.attacks.extraction import CopycatCNN\n\nnp.random.seed(42)\nprint(\"All tools imported successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üìÇ Step 2: Load the Dataset and Train the Victim Model\n\nWe reuse the Nursery dataset from Lab 3. This time we train a more powerful victim model ‚Äî the expensive proprietary model the attacker wants to steal.\n\nFrom this point forward we pretend we have no access to this model except through an API that takes inputs and returns predictions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LOAD AND PREPARE THE NURSERY DATASET\n# =============================================================================\n\ncolumn_names = [\n    'parents', 'has_nurs', 'form', 'children',\n    'housing', 'finance', 'social', 'health', 'target'\n]\n\ndf = pd.read_csv(\n    '../datasets/nursery.data',\n    header=None,\n    names=column_names\n)\n\ndf_encoded = df.copy()\nfor column in df_encoded.columns:\n    le = LabelEncoder()\n    df_encoded[column] = le.fit_transform(df_encoded[column])\n\nX = df_encoded.drop('target', axis=1).values\ny = df_encoded['target'].values\n\n# Split into three portions:\n#   X_train      : victim model trains on this (attacker never sees this)\n#   X_steal_pool : attacker uses this to query the victim and collect responses\n#   X_eval       : used to evaluate both models fairly\nX_train, X_remaining, y_train, y_remaining = train_test_split(\n    X, y, test_size=0.5, random_state=42, stratify=y\n)\nX_steal_pool, X_eval, y_steal_pool, y_eval = train_test_split(\n    X_remaining, y_remaining, test_size=0.4, random_state=42\n)\n\nprint(\"Data prepared:\")\nprint(f\"  Victim model training data : {len(X_train)} records (attacker CANNOT see this)\")\nprint(f\"  Attacker query pool        : {len(X_steal_pool)} records\")\nprint(f\"  Evaluation set             : {len(X_eval)} records\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAIN THE VICTIM MODEL\n# =============================================================================\n# This is the valuable proprietary model. In a real scenario it sits\n# behind an API. The attacker cannot see its code, weights, or training data.\n# =============================================================================\n\nprint(\"Training the VICTIM model (200 trees - may take 20-30 seconds)...\")\nprint(\"\")\n\nvictim_model = RandomForestClassifier(n_estimators=200, random_state=42)\nvictim_model.fit(X_train, y_train)\n\nvictim_accuracy = accuracy_score(y_eval, victim_model.predict(X_eval))\n\nprint(f\"Victim model accuracy on evaluation set: {victim_accuracy*100:.2f}%\")\nprint(\"\")\nprint(\"This is our benchmark. The stolen model will try to match it.\")\nprint(\"From here the attacker only has API access - no model internals.\")\n\nart_victim = SklearnClassifier(model=victim_model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- The victim model's accuracy is our benchmark.\n- The attacker's goal is to match this as closely as possible.\n- Remember: from this point the attacker has NO access to training data or model code.\n\n---\n\n## üî¥ Step 3: Perform the Extraction Attack\n\nThe attack works like this:\n1. Attacker sends records from their query pool to the victim's API\n2. Gets back predictions\n3. Now has (input, label) pairs ‚Äî but labels came from the victim, not ground truth\n4. Trains their own model on this stolen dataset\n\nThe victim model is being used as a labeling service."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EXTRACTION ATTACK AT THREE QUERY BUDGETS\n# =============================================================================\n# We test with 100, 500, and 2000 queries to show how quality scales.\n# More queries = better stolen model but also more suspicious to the defender.\n# =============================================================================\n\ndef create_stolen_model():\n    \"\"\"Creates a fresh logistic regression model to use as the copycat.\n    The attacker does not need to use the same model type as the victim.\"\"\"\n    return SklearnClassifier(\n        model=LogisticRegression(max_iter=1000, random_state=42)\n    )\n\nquery_budgets = [100, 500, 2000]\nstolen_results = []\n\nprint(\"Running extraction attack with different query budgets...\")\nprint(\"(More queries = longer runtime)\")\nprint(\"\")\n\nfor n_queries in query_budgets:\n    print(f\"  Testing with {n_queries} queries...\")\n\n    stolen_classifier = create_stolen_model()\n\n    attack = CopycatCNN(\n        classifier=art_victim,\n        batch_size_fit=32,\n        batch_size_query=32,\n        nb_epochs=10,\n        nb_stolen=n_queries\n    )\n\n    # extract() queries the victim and trains the stolen model\n    stolen_model = attack.extract(\n        x=X_steal_pool[:n_queries],\n        y=y_steal_pool[:n_queries],\n        thieved_classifier=stolen_classifier\n    )\n\n    stolen_preds = stolen_model.predict(X_eval)\n    stolen_labels = np.argmax(stolen_preds, axis=1)\n    stolen_accuracy = accuracy_score(y_eval, stolen_labels)\n\n    victim_preds = victim_model.predict(X_eval)\n    agreement = accuracy_score(victim_preds, stolen_labels)\n\n    stolen_results.append((n_queries, stolen_accuracy, agreement))\n    print(f\"    Accuracy: {stolen_accuracy*100:.2f}% | Agreement with victim: {agreement*100:.2f}%\")\n\nprint(\"\")\nprint(f\"Victim model accuracy (benchmark): {victim_accuracy*100:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- **Accuracy** ‚Äî how well the stolen model performs on the task overall\n- **Agreement** ‚Äî how often stolen and victim give the same answer\n- Does more queries always improve the stolen model?\n- Even with only 100 queries, how close did the stolen model get?\n\n---\n\n## üìä Step 4: Visualise the Trade-off"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "budgets = [r[0] for r in stolen_results]\naccuracies = [r[1]*100 for r in stolen_results]\nagreements = [r[2]*100 for r in stolen_results]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.plot(budgets, accuracies, 'bo-', linewidth=2, markersize=8)\nax1.axhline(y=victim_accuracy*100, color='red', linestyle='--',\n            label=f'Victim ({victim_accuracy*100:.1f}%)')\nax1.set_title('Stolen Model Accuracy vs Query Budget')\nax1.set_xlabel('Number of Queries')\nax1.set_ylabel('Accuracy (%)')\nax1.legend()\nax1.grid(True, alpha=0.3)\nfor b, a in zip(budgets, accuracies):\n    ax1.annotate(f'{a:.1f}%', (b, a), textcoords=\"offset points\", xytext=(0,10))\n\nax2.plot(budgets, agreements, 'go-', linewidth=2, markersize=8)\nax2.axhline(y=100, color='red', linestyle='--', label='Perfect copy (100%)')\nax2.set_title('Stolen Model Agreement with Victim vs Query Budget')\nax2.set_xlabel('Number of Queries')\nax2.set_ylabel('Agreement (%)')\nax2.legend()\nax2.grid(True, alpha=0.3)\nfor b, a in zip(budgets, agreements):\n    ax2.annotate(f'{a:.1f}%', (b, a), textcoords=\"offset points\", xytext=(0,10))\n\nplt.tight_layout()\nplt.savefig('../outputs/lab4_extraction_results.png')\nplt.show()\nprint(\"Chart saved to outputs folder.\")\nprint(\"\")\nprint(\"Summary:\")\nprint(\"=\" * 55)\nprint(f\"{'Queries':<12} {'Stolen Accuracy':<20} {'Agreement with Victim'}\")\nprint(\"-\" * 55)\nfor budget, acc, agr in stolen_results:\n    print(f\"{budget:<12} {acc*100:.2f}%{'':<14} {agr*100:.2f}%\")\nprint(\"-\" * 55)\nprint(f\"{'Victim':<12} {victim_accuracy*100:.2f}% (benchmark)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- Notice **diminishing returns** ‚Äî improvement from 100 to 500 queries may be much larger than from 500 to 2000. Why does this happen?\n- Is 80%+ agreement a successful steal?\n\n### üß™ Try This\n\nAdd `50` as the first value in `query_budgets` and rerun.\n\n- Can you build a usable stolen model with just 50 queries?\n- From a defender's perspective, at what query count should a security alert trigger?\n\n---\n\n## üõ°Ô∏è Step 5: Think Like a Defender"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DEFENSIVE ANALYSIS\n# =============================================================================\n# If you were protecting the victim model, what signals would alert you\n# to an extraction attack in progress?\n# =============================================================================\n\nprint(\"DEFENDER PERSPECTIVE\")\nprint(\"=\" * 65)\nprint(\"\")\nprint(\"1. QUERY VOLUME MONITORING\")\nprint(\"-\" * 40)\nfor n_queries, acc, agr in stolen_results:\n    print(f\"   {n_queries:>5} queries achieved {agr*100:.1f}% agreement with victim\")\nprint(\"   -> Set an alert threshold on query volume per user/IP\")\nprint(\"\")\n\nprint(\"2. OUTPUT ROUNDING DEFENSE\")\nprint(\"-\" * 40)\nsample = X_eval[:2]\nexact_probs = victim_model.predict_proba(sample)\nrounded_probs = np.round(exact_probs, 1)\n\nprint(\"   Exact probabilities (full info to attacker):\")\nfor i, p in enumerate(exact_probs):\n    top3 = sorted(zip(p, range(len(p))), reverse=True)[:3]\n    print(f\"   Record {i+1}: {[round(v,4) for v,_ in top3]} (top 3 classes)\")\n\nprint(\"\")\nprint(\"   Rounded to 1 decimal (less info):\")\nfor i, p in enumerate(rounded_probs):\n    top3 = sorted(zip(p, range(len(p))), reverse=True)[:3]\n    print(f\"   Record {i+1}: {[round(v,1) for v,_ in top3]} (top 3 classes)\")\n\nprint(\"\")\nprint(\"   -> Less precision = less useful to attacker = worse stolen model\")\nprint(\"\")\nprint(\"3. LABEL-ONLY DEFENSE\")\nprint(\"-\" * 40)\nprint(\"   Return ONLY the predicted class, no probabilities at all.\")\nprint(\"   Forces attacker to need far more queries for same quality.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üëÄ What Do You See?\n\n- Which defense do you think would be most effective?\n- Which would be least disruptive to legitimate users?\n- Real companies like Google and Amazon expose ML via APIs. What defenses do you think they use?\n\n---\n\n## üí≠ Step 6: Reflect"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "reflection = \"\"\"\nLAB 4 - EXTRACTION ATTACK REFLECTION\n======================================\n\nQ1: In plain English, what is a model extraction attack and what does\n    the attacker gain?\nA1: [TYPE YOUR ANSWER HERE]\n\nQ2: The attacker only queried the API ‚Äî never saw model code or training data.\n    What does this tell you about the risk of exposing ML models via public APIs?\nA2: [TYPE YOUR ANSWER HERE]\n\nQ3: You saw diminishing returns as query count increased. Why does the\n    stolen model improve quickly at first, then plateau?\nA3: [TYPE YOUR ANSWER HERE]\n\nQ4: Rank the three defenses (query limiting, output rounding, label-only)\n    from most to least effective. Explain your reasoning.\nA4: [TYPE YOUR ANSWER HERE]\n\nQ5: Looking back at all four labs, which attack poses the greatest risk\n    to organisations deploying AI today? Justify your answer.\nA5: [TYPE YOUR ANSWER HERE]\n\nBONUS: Can you think of a scenario where a model extraction attack could\n       be used for ethically justified reasons?\nBONUS: [TYPE YOUR ANSWER HERE]\n\"\"\"\n\nwith open('../outputs/Lab4_Reflection.txt', 'w') as f:\n    f.write(reflection)\n\nprint(\"Reflection saved to outputs/Lab4_Reflection.txt\")\nprint(reflection)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ‚úÖ Lab 4 Complete ‚Äî And So Is the Course!\n\nYou have worked through all four core attack types:\n\n| Attack | When | Target | Key Tool |\n|--------|------|--------|----------|\n| Evasion | After deployment | Model inputs | HopSkipJump (ART) |\n| Poisoning | During training | Training data | Label Flipping |\n| Inference | After deployment | Data privacy | MembershipInference (ART) |\n| Extraction | After deployment | Model IP | CopycatCNN (ART) |\n\nEach attack represents a real threat that AI security practitioners defend against today. The tools you used are the same tools used by researchers at IBM, Microsoft, Google, and security firms worldwide.\n\nReturn to [START_HERE.ipynb](START_HERE.ipynb) to review your completed labs.\n\n---\n*Built with the Adversarial Robustness Toolbox (ART) ‚Äî https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
  }
 ]
}