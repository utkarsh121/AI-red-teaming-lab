{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ Lab 4 ‚Äî Extraction Attack (Model Stealing)\n",
    "### Certified AI Penetration Tester ‚Äì Red Team (CAIPT-RT)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Story\n",
    "\n",
    "A company has spent years and millions of dollars building a machine learning model that predicts whether families qualify for social services. The model is their competitive advantage. They protect it carefully ‚Äî the model itself, the code, and the training data are all kept secret.\n",
    "\n",
    "But they do offer it as an API service. You send it an application, it sends back a decision. That is all you get ‚Äî no code, no probabilities, just a label.\n",
    "\n",
    "You are an attacker ‚Äî perhaps a competitor, perhaps a researcher exposing bias. You have no access to the model internals or the training data. But you do have access to the API.\n",
    "\n",
    "By sending thousands of carefully chosen queries and recording the responses, you can **build your own model that behaves almost identically** to the original ‚Äî without ever seeing it.\n",
    "\n",
    "This is a **Model Extraction Attack** ‚Äî also called model stealing.\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ What is a Model Extraction Attack?\n",
    "\n",
    "A model extraction attack allows an attacker to create a functional copy of a machine learning model by repeatedly querying it and using the query-response pairs as training data for a new model.\n",
    "\n",
    "**Why is this a problem?**\n",
    "- The stolen model can be used to **steal intellectual property** ‚Äî years of R&D reproduced for free\n",
    "- The stolen model can be used to **prepare better attacks** ‚Äî once you have a local copy, you can run evasion and poisoning attacks against it much more effectively\n",
    "- The stolen model can be used to **probe for bias** ‚Äî sometimes used by researchers to expose unfairness in proprietary models\n",
    "\n",
    "**Real world examples:**\n",
    "- Stealing a competitor's fraud detection model\n",
    "- Copying a medical diagnosis model to avoid licensing fees\n",
    "- Using a stolen model as a stepping stone for further attacks\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è What We Will Do in This Lab\n",
    "\n",
    "1. Train the \"victim\" model ‚Äî the valuable model being stolen\n",
    "2. Set up a query interface simulating black-box API access\n",
    "3. Use ART's extraction attack to steal the model\n",
    "4. Evaluate how close the stolen model is to the original\n",
    "5. Test how query volume affects the quality of the stolen model\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 1: Import the Tools We Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "# New addition for this lab:\n",
    "# CopycatCNN : ART's model extraction attack\n",
    "#              Despite the name 'CNN' (Convolutional Neural Network),\n",
    "#              this attack works for any classifier - the name comes\n",
    "#              from the original research paper that introduced this\n",
    "#              technique for image models, but ART adapted it broadly.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ART extraction attack\n",
    "from art.estimators.classification import SklearnClassifier\n",
    "from art.attacks.extraction import CopycatCNN\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All tools imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÇ Step 2: Load the Dataset and Train the Victim Model\n",
    "\n",
    "We reuse the Nursery dataset from Lab 3. This time, we train a more powerful victim model ‚Äî the \"expensive proprietary model\" that the attacker wants to steal.\n",
    "\n",
    "We then pretend we have no access to this model except through an API that takes inputs and returns predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD AND PREPARE THE NURSERY DATASET\n",
    "# =============================================================================\n",
    "# Same loading process as Lab 3\n",
    "# =============================================================================\n",
    "\n",
    "column_names = [\n",
    "    'parents', 'has_nurs', 'form', 'children',\n",
    "    'housing', 'finance', 'social', 'health', 'target'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '../datasets/nursery.data',\n",
    "    header=None,\n",
    "    names=column_names\n",
    ")\n",
    "\n",
    "# Encode all text columns to numbers\n",
    "df_encoded = df.copy()\n",
    "for column in df_encoded.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[column] = le.fit_transform(df_encoded[column])\n",
    "\n",
    "X = df_encoded.drop('target', axis=1).values\n",
    "y = df_encoded['target'].values\n",
    "\n",
    "# Split the data\n",
    "# Note: we keep a separate 'steal_pool' dataset\n",
    "# This represents data the ATTACKER has access to (not the original training data)\n",
    "# The attacker uses this pool to query the victim model and collect responses\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split remaining data into attacker's query pool and evaluation set\n",
    "X_steal_pool, X_eval, y_steal_pool, y_eval = train_test_split(\n",
    "    X_remaining, y_remaining, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data prepared:\")\n",
    "print(f\"  Victim model training data : {len(X_train)} records\")\n",
    "print(f\"  Attacker query pool        : {len(X_steal_pool)} records\")\n",
    "print(f\"  Evaluation set             : {len(X_eval)} records\")\n",
    "print(\"\")\n",
    "print(\"The attacker ONLY has access to the query pool.\")\n",
    "print(\"The attacker does NOT have the training data or original labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN THE VICTIM MODEL\n",
    "# =============================================================================\n",
    "# This is the valuable, proprietary model the attacker wants to steal.\n",
    "# In a real scenario, this model would sit behind an API.\n",
    "# The attacker cannot see its code, weights, or training data.\n",
    "#\n",
    "# We use a Random Forest with 200 trees - a strong, well-trained model\n",
    "# that represents real-world production quality\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training the VICTIM model (the valuable model to be stolen)...\")\n",
    "print(\"(200 decision trees - may take 20-30 seconds)\")\n",
    "print(\"\")\n",
    "\n",
    "victim_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "victim_model.fit(X_train, y_train)\n",
    "\n",
    "victim_accuracy = accuracy_score(y_eval, victim_model.predict(X_eval))\n",
    "\n",
    "print(f\"Victim model trained successfully.\")\n",
    "print(f\"Victim model accuracy on evaluation set: {victim_accuracy*100:.2f}%\")\n",
    "print(\"\")\n",
    "print(\"This is the accuracy benchmark. The stolen model will try to match this.\")\n",
    "\n",
    "# Wrap in ART\n",
    "art_victim = SklearnClassifier(model=victim_model)\n",
    "print(\"\")\n",
    "print(\"Victim model wrapped in ART. Simulating API access only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "- The victim model's accuracy on the evaluation set is our benchmark.\n",
    "- The attacker's goal is to build a stolen model that performs as close to this as possible.\n",
    "- Remember: from this point forward, the attacker has NO access to the victim model's code or training data ‚Äî only the ability to query it.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¥ Step 3: Perform the Extraction Attack\n",
    "\n",
    "The extraction attack works like this:\n",
    "\n",
    "1. The attacker takes data from their own query pool\n",
    "2. They send each record to the victim model's API and get back a prediction\n",
    "3. They now have a dataset of (input, label) pairs ‚Äî but the labels came from the victim model, not the original data\n",
    "4. They train their own model on this \"stolen\" dataset\n",
    "\n",
    "This is essentially using the victim model as a labeling service to create training data for the copycat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXTRACTION ATTACK - STEAL THE MODEL\n",
    "# =============================================================================\n",
    "# CopycatCNN parameters:\n",
    "#\n",
    "# classifier      : the victim model being stolen (via ART wrapper)\n",
    "# batch_size_fit  : how many samples to use per training batch\n",
    "# batch_size_query: how many samples to query the victim model at once\n",
    "# nb_epochs       : how many times to train the stolen model on the collected data\n",
    "# nb_stolen       : CRITICAL - how many queries to send to the victim model\n",
    "#                   More queries = more data = better stolen model\n",
    "#                   But more queries also = more suspicious to the victim owner\n",
    "#\n",
    "# We will run THREE versions with different query budgets to see the trade-off\n",
    "# =============================================================================\n",
    "\n",
    "# The stolen model architecture - we use Logistic Regression as the copycat\n",
    "# The attacker does not need to use the same model type as the victim\n",
    "def create_stolen_model():\n",
    "    return SklearnClassifier(\n",
    "        model=LogisticRegression(max_iter=1000, random_state=42)\n",
    "    )\n",
    "\n",
    "query_budgets = [100, 500, 2000]\n",
    "stolen_results = []\n",
    "\n",
    "print(\"Running extraction attack with different query budgets...\")\n",
    "print(\"(More queries = longer runtime but potentially better stolen model)\")\n",
    "print(\"\")\n",
    "\n",
    "for n_queries in query_budgets:\n",
    "    print(f\"  Testing with {n_queries} queries...\")\n",
    "    \n",
    "    # Create a fresh stolen model for each test\n",
    "    stolen_classifier = create_stolen_model()\n",
    "    \n",
    "    # Create and run the extraction attack\n",
    "    attack = CopycatCNN(\n",
    "        classifier=art_victim,\n",
    "        batch_size_fit=32,\n",
    "        batch_size_query=32,\n",
    "        nb_epochs=10,\n",
    "        nb_stolen=n_queries\n",
    "    )\n",
    "    \n",
    "    # extract() is where the stealing happens\n",
    "    # It queries the victim model and trains the stolen model\n",
    "    stolen_model = attack.extract(\n",
    "        x=X_steal_pool[:n_queries],\n",
    "        y=y_steal_pool[:n_queries],\n",
    "        thieved_classifier=stolen_classifier\n",
    "    )\n",
    "    \n",
    "    # Evaluate the stolen model\n",
    "    stolen_preds = stolen_model.predict(X_eval)\n",
    "    stolen_labels = np.argmax(stolen_preds, axis=1)\n",
    "    stolen_accuracy = accuracy_score(y_eval, stolen_labels)\n",
    "    \n",
    "    # Also check agreement with victim model (not just accuracy)\n",
    "    # Agreement = how often stolen model gives SAME answer as victim\n",
    "    victim_preds = victim_model.predict(X_eval)\n",
    "    agreement = accuracy_score(victim_preds, stolen_labels)\n",
    "    \n",
    "    stolen_results.append((n_queries, stolen_accuracy, agreement))\n",
    "    print(f\"    Accuracy: {stolen_accuracy*100:.2f}% | Agreement with victim: {agreement*100:.2f}%\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Victim model accuracy (benchmark): {victim_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "Look at the results for each query budget.\n",
    "\n",
    "- **Accuracy** tells you how well the stolen model performs on the task overall.\n",
    "- **Agreement** tells you how often the stolen model gives the same answer as the victim ‚Äî this is a measure of how faithful the copy is.\n",
    "- Does increasing the number of queries always improve the stolen model? At what point does adding more queries stop helping significantly?\n",
    "- Even with only 100 queries, how close did the stolen model get to the victim?\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Step 4: Visualize the Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE QUERY BUDGET VS STOLEN MODEL QUALITY\n",
    "# =============================================================================\n",
    "\n",
    "budgets = [r[0] for r in stolen_results]\n",
    "accuracies = [r[1]*100 for r in stolen_results]\n",
    "agreements = [r[2]*100 for r in stolen_results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs Query Budget\n",
    "ax1.plot(budgets, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=victim_accuracy*100, color='red', linestyle='--',\n",
    "           label=f'Victim accuracy ({victim_accuracy*100:.1f}%)')\n",
    "ax1.set_title('Stolen Model Accuracy vs Query Budget')\n",
    "ax1.set_xlabel('Number of Queries to Victim Model')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "for b, a in zip(budgets, accuracies):\n",
    "    ax1.annotate(f'{a:.1f}%', (b, a), textcoords=\"offset points\", xytext=(0, 10))\n",
    "\n",
    "# Plot 2: Agreement vs Query Budget\n",
    "ax2.plot(budgets, agreements, 'go-', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=100, color='red', linestyle='--', label='Perfect copy (100%)')\n",
    "ax2.set_title('Stolen Model Agreement with Victim vs Query Budget')\n",
    "ax2.set_xlabel('Number of Queries to Victim Model')\n",
    "ax2.set_ylabel('Agreement with Victim Model (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "for b, a in zip(budgets, agreements):\n",
    "    ax2.annotate(f'{a:.1f}%', (b, a), textcoords=\"offset points\", xytext=(0, 10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/lab4_extraction_results.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved to outputs folder.\")\n",
    "print(\"\")\n",
    "print(\"Summary Table:\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Queries':<12} {'Stolen Accuracy':<20} {'Agreement with Victim'}\")\n",
    "print(\"-\" * 55)\n",
    "for budget, acc, agr in stolen_results:\n",
    "    print(f\"{budget:<12} {acc:.2f}%{'':<14} {agr:.2f}%\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Victim':<12} {victim_accuracy*100:.2f}% (benchmark)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "- Look at the accuracy chart. How close does the stolen model get to the victim's accuracy?\n",
    "- Look at the agreement chart. Agreement of 80%+ means 4 out of 5 predictions match the victim. Is that a successful steal?\n",
    "- Notice that the improvement from 100 to 500 queries might be much larger than from 500 to 2000 queries. This is called **diminishing returns**. Why does this happen?\n",
    "\n",
    "### üß™ Try This\n",
    "\n",
    "Edit the `query_budgets` list at the top of the attack cell and add `50` as the first value.\n",
    "\n",
    "- Can you build a usable stolen model with just 50 queries?\n",
    "- From a defender's perspective, what is the minimum number of API queries that should trigger a security alert?\n",
    "\n",
    "---\n",
    "\n",
    "## üõ°Ô∏è Step 5: Think Like a Defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEFENSIVE ANALYSIS\n",
    "# =============================================================================\n",
    "# Now we switch perspective. If you were protecting the victim model,\n",
    "# what signals would tell you that someone is trying to steal it?\n",
    "#\n",
    "# This cell simulates what a defender might monitor:\n",
    "# - Query volume (too many queries = suspicious)\n",
    "# - Query patterns (systematic coverage of input space = suspicious)\n",
    "# - Output rounding (reduce information by rounding probabilities)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"DEFENDER PERSPECTIVE: What would alert you to an extraction attack?\")\n",
    "print(\"=\" * 65)\n",
    "print(\"\")\n",
    "\n",
    "# Simulate what the attacker's queries look like from the defender's view\n",
    "print(\"1. QUERY VOLUME ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "for n_queries, acc, agr in stolen_results:\n",
    "    print(f\"   Attack using {n_queries:>5} queries achieved {agr:.1f}% agreement\")\n",
    "print(\"   -> A defender monitoring query volume could set an alert threshold\")\n",
    "print(\"\")\n",
    "\n",
    "# Demonstrate output rounding as a defense\n",
    "print(\"2. OUTPUT ROUNDING DEFENSE\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   Instead of returning exact probabilities, round them to 2 decimal places.\")\n",
    "print(\"   This reduces the information the attacker gets per query.\")\n",
    "print(\"\")\n",
    "\n",
    "# Show the difference in information\n",
    "sample = X_eval[:3]\n",
    "exact_probs = victim_model.predict_proba(sample)\n",
    "rounded_probs = np.round(exact_probs, 2)\n",
    "\n",
    "print(\"   Exact probabilities (what attacker gets without defense):\")\n",
    "for i, p in enumerate(exact_probs):\n",
    "    print(f\"   Record {i+1}: {p}\")\n",
    "print(\"\")\n",
    "print(\"   Rounded probabilities (with rounding defense):\")\n",
    "for i, p in enumerate(rounded_probs):\n",
    "    print(f\"   Record {i+1}: {p}\")\n",
    "print(\"\")\n",
    "print(\"   -> Less precise = less useful to attacker = worse stolen model\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"3. LABEL-ONLY DEFENSE\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   Return ONLY the predicted class label, no probabilities at all.\")\n",
    "print(\"   This forces the attacker to work with much less information.\")\n",
    "print(\"   The attack can still work but requires far more queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ What Do You See?\n",
    "\n",
    "- The defender has several options to make extraction harder without completely shutting down API access.\n",
    "- Which defense do you think would be most effective? Which would be least disruptive to legitimate users?\n",
    "- In real life, companies like Google and Amazon expose ML models through APIs. What defenses do you think they use?\n",
    "\n",
    "---\n",
    "\n",
    "## üí≠ Step 6: Reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REFLECTION - SAVE YOUR ANSWERS\n",
    "# =============================================================================\n",
    "\n",
    "reflection = \"\"\"\n",
    "LAB 4 - EXTRACTION ATTACK REFLECTION\n",
    "======================================\n",
    "\n",
    "Q1: In plain English, what is a model extraction attack?\n",
    "    What does the attacker gain that they did not have before?\n",
    "A1: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q2: The attacker only needed to query the API - they never saw the model\n",
    "    code, weights, or training data. What does this tell you about the\n",
    "    risk of exposing ML models through public APIs?\n",
    "A2: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q3: You saw diminishing returns as query count increased.\n",
    "    Why does the stolen model improve quickly at first, then plateau?\n",
    "    (Think about what information each new query adds)\n",
    "A3: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q4: You learned three defensive approaches: query rate limiting,\n",
    "    output rounding, and label-only responses.\n",
    "    Rank these from most to least effective and explain your reasoning.\n",
    "A4: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "Q5: Looking back at all four labs, which attack do you think poses\n",
    "    the greatest risk to organizations deploying AI systems today?\n",
    "    Justify your answer.\n",
    "A5: [TYPE YOUR ANSWER HERE]\n",
    "\n",
    "BONUS: Can you think of a scenario where a model extraction attack\n",
    "       could actually be used for GOOD (ethically justified reasons)?\n",
    "BONUS ANSWER: [TYPE YOUR ANSWER HERE]\n",
    "\"\"\"\n",
    "\n",
    "with open('../outputs/Lab4_Reflection.txt', 'w') as f:\n",
    "    f.write(reflection)\n",
    "\n",
    "print(\"Reflection saved to outputs/Lab4_Reflection.txt\")\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Lab 4 Complete ‚Äî And So Is the Course!\n",
    "\n",
    "You have successfully:\n",
    "- Trained a victim model representing a valuable proprietary system\n",
    "- Performed a model extraction attack using only API query access\n",
    "- Measured how query volume affects stolen model quality\n",
    "- Analyzed the trade-off between attack cost and stolen model fidelity\n",
    "- Explored defensive countermeasures from the defender's perspective\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Course Summary ‚Äî What You Have Learned\n",
    "\n",
    "You have now performed all four core attack types against machine learning systems:\n",
    "\n",
    "| Attack | When It Happens | What Is Targeted | Key Tool Used |\n",
    "|--------|----------------|------------------|---------------|\n",
    "| Evasion | After deployment | Model inputs | HopSkipJump (ART) |\n",
    "| Poisoning | During training | Training data | Label Flipping |\n",
    "| Inference | After deployment | Training data privacy | MembershipInference (ART) |\n",
    "| Extraction | After deployment | Model IP | CopycatCNN (ART) |\n",
    "\n",
    "Each of these attacks represents a real threat that AI security practitioners are defending against today. The tools you used ‚Äî particularly the Adversarial Robustness Toolbox ‚Äî are the same tools used by researchers at IBM, Microsoft, Google, and security firms around the world.\n",
    "\n",
    "Return to [START_HERE.ipynb](START_HERE.ipynb) to review your completed labs.\n",
    "\n",
    "---\n",
    "*Lab built with the Adversarial Robustness Toolbox (ART)*  \n",
    "*https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
