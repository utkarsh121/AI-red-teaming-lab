{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸ”´ Lab 4 â€” Extraction Attack (Model Stealing)\n### Certified AI Penetration Tester â€“ Red Team (CAIPT-RT)\n\n---\n\n## ðŸŽ¯ The Story\n\nA company spent three years and significant budget building a machine learning model that predicts whether families qualify for social services. The model is their competitive advantage â€” it is more accurate than anything else on the market. The code, internal weights, and training data are all kept completely secret.\n\nBut they offer it as a paid API service. You send it an application profile, it sends back a decision. That is all you get â€” a prediction.\n\nYou are an attacker â€” perhaps a competitor, perhaps a researcher trying to expose bias in an opaque system. You have no access to the model internals whatsoever. But you have access to the API.\n\nBy sending thousands of carefully chosen queries and recording every response, you can **build your own model that makes almost identical decisions** to the original â€” without ever seeing the training data or the model code.\n\nThis is a **Model Extraction Attack** â€” also called model stealing.\n\n---\n\n## ðŸ“– What is a Model Extraction Attack?\n\nThe attacker creates a functional copy of a model by repeatedly querying it and using the query-response pairs as their own training data.\n\n**The core idea:** Every time you ask the model a question and it answers, it is unknowingly teaching you how it thinks. Collect enough answers and you can train your own model to think the same way.\n\n**Why is this a problem?**\n- **Intellectual property theft** â€” years of R&D reproduced for free\n- **Enabling further attacks** â€” a stolen local copy can be attacked using white-box methods that require direct model access\n- **Exposing bias** â€” researchers sometimes use extraction to audit black-box models for fairness\n\n**Real world examples:**\n- Stealing a competitor's fraud detection model to avoid licensing fees\n- Copying a medical diagnosis model and rebranding it\n- Using a stolen model as a stepping stone to prepare evasion attacks (Lab 1)\n\n---\n\n## ðŸ—‚ï¸ What We Will Do in This Lab\n\n1. Load the Nursery dataset and prepare it for modelling\n2. Train the victim model â€” the valuable proprietary model being stolen\n3. Set up a black-box query interface that simulates API access only\n4. Query the victim and collect its responses as stolen training data\n5. Train our own model on those responses and measure how close we got\n6. Compare results at different query budgets â€” more queries, better copy?\n7. Think like a defender â€” what signals would alert you to this attack?\n\n---\n\n## âš™ï¸ Step 1: Import the Tools We Need"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# IMPORTS\n# =============================================================================\n# numpy      : handles numbers and arrays\n# pandas     : loads and manages datasets\n# matplotlib : draws charts to visualise results\n# sklearn    : builds and trains machine learning models\n# art        : Adversarial Robustness Toolbox â€” wraps models for attacks\n#\n# Note: CopycatCNN is imported for its querying infrastructure only.\n# We do not use its internal training step because it passes neural-network\n# specific parameters (batch_size, nb_epochs) that sklearn models reject.\n# Instead we query the victim through ART and train our stolen model manually.\n# This is explained in detail in the attack step below.\n# =============================================================================\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom art.estimators.classification import SklearnClassifier\n\nnp.random.seed(42)\nprint(\"All tools imported successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ðŸ“‚ Step 2: Load the Dataset and Create Three Data Splits\n\nWe reuse the Nursery dataset from Lab 3. This time we split it into **three portions** rather than two, because we have three distinct roles to play:\n\n**Victim's training data** â€” the private data the victim model learns from. The attacker never sees this. In a real scenario this would be the company's proprietary dataset stored securely on their servers.\n\n**Attacker's query pool** â€” data the attacker uses to send queries to the victim's API. These are records the attacker has access to independently â€” they do not need to come from the victim's dataset. Any plausible input data will do.\n\n**Evaluation set** â€” held out completely and used only to measure the quality of both the victim and stolen models fairly. Neither model trained on this data.\n\nThis three-way split mirrors a real attack scenario where the attacker has their own data, the victim has their own private data, and we measure everything on neutral ground."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LOAD AND ENCODE THE NURSERY DATASET\n# =============================================================================\n\ncolumn_names = [\n    'parents', 'has_nurs', 'form', 'children',\n    'housing', 'finance', 'social', 'health', 'target'\n]\n\ndf = pd.read_csv(\n    '../datasets/nursery.data',\n    header=None,\n    names=column_names\n)\n\n# Convert all text values to integers (same as Lab 3)\ndf_encoded = df.copy()\nfor column in df_encoded.columns:\n    le = LabelEncoder()\n    df_encoded[column] = le.fit_transform(df_encoded[column])\n\nX = df_encoded.drop('target', axis=1).values\ny = df_encoded['target'].values\n\nprint(f\"Dataset loaded: {len(df):,} records  |  {X.shape[1]} features  |  {len(np.unique(y))} target classes\")\nprint(\"\")\n\n# =============================================================================\n# THREE-WAY SPLIT\n# =============================================================================\n# Split 1: 50% for victim training, 50% remaining\n# Split 2: of the remaining 50%, take 60% for attacker queries and 40% for eval\n#\n# Result:\n#   X_train      : 50% of data  (victim trains on this â€” attacker CANNOT see it)\n#   X_steal_pool : 30% of data  (attacker queries the victim with these)\n#   X_eval       : 20% of data  (neutral evaluation set â€” neither model trained here)\n# =============================================================================\n\nX_train, X_remaining, y_train, y_remaining = train_test_split(\n    X, y, test_size=0.5, random_state=42, stratify=y\n)\nX_steal_pool, X_eval, y_steal_pool, y_eval = train_test_split(\n    X_remaining, y_remaining, test_size=0.4, random_state=42\n)\n\nprint(\"Three-way data split complete:\")\nprint(f\"  Victim training data (PRIVATE) : {len(X_train):>5,} records  <- attacker never sees this\")\nprint(f\"  Attacker query pool            : {len(X_steal_pool):>5,} records  <- attacker queries victim with these\")\nprint(f\"  Neutral evaluation set         : {len(X_eval):>5,} records  <- used to score both models fairly\")\nprint(\"\")\nprint(f\"  Max queries available to attacker: {len(X_steal_pool):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TRAIN THE VICTIM MODEL\n# =============================================================================\n# This is the valuable proprietary model the attacker wants to steal.\n# It is a powerful Random Forest with 200 trees.\n#\n# In a real scenario this model sits behind a paid API. The attacker\n# cannot see its code, weights, hyperparameters, or training data.\n# All they can do is send an input and receive a prediction back.\n#\n# We wrap it in ART's SklearnClassifier so that when we call\n# art_victim.predict(), it returns probability distributions (one per class)\n# rather than just hard labels. This is what a real API might return.\n# =============================================================================\n\nprint(\"Training victim model (200 trees)...\")\nprint(\"(May take 20-30 seconds)\")\nprint(\"\")\n\nvictim_model = RandomForestClassifier(n_estimators=200, random_state=42)\nvictim_model.fit(X_train, y_train)\n\nvictim_accuracy = accuracy_score(y_eval, victim_model.predict(X_eval))\n\n# Wrap in ART so predict() returns probability arrays not just labels\nart_victim = SklearnClassifier(model=victim_model)\n\nprint(f\"Victim model accuracy on neutral eval set: {victim_accuracy*100:.2f}%\")\nprint(\"\")\nprint(\"This is our benchmark target.\")\nprint(\"The stolen model will try to get as close to this as possible.\")\nprint(\"\")\n\n# Demonstrate what the API returns for a single query\nsample_query  = X_eval[:1]\nsample_probs  = art_victim.predict(sample_query)\nsample_label  = victim_model.predict(sample_query)[0]\n\nprint(\"What the attacker sees for one query (the API response):\")\nprint(f\"  Input record  : {sample_query[0]}\")\nprint(f\"  Probabilities : {[round(p, 4) for p in sample_probs[0]]}\")\nprint(f\"  Predicted class: {sample_label}\")\nprint(\"\")\nprint(\"The attacker cannot see the model code or weights â€” only this output.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\nThe victim model gives back a **probability distribution** for each query â€” a list of numbers that sum to 1.0, one per class. The highest number is the predicted class.\n\n**Why do probabilities matter more than just the label?**\n\nIf the API only returned a label â€” say `class 2` â€” the attacker gets limited information. But if it returns `[0.01, 0.02, 0.94, 0.02, 0.01]`, the attacker knows the model is 94% confident it is class 2. That confidence information is extremely valuable for training a stolen model, because the stolen model can try to match not just which class was predicted, but *how sure* the victim was.\n\nThis is why the \"label-only\" defense is effective â€” returning just the class name strips out all the confidence information.\n\n---\n\n## ðŸ”´ Step 3: Perform the Extraction Attack\n\nHere is exactly how model stealing works in this lab:\n\n**Phase 1 â€” Query the victim (information gathering)**\n\nThe attacker sends records from their query pool to the victim model's API. For each record, the victim returns a probability distribution. The attacker collects all these responses. They now have a dataset of (input, victim_prediction) pairs â€” a stolen dataset.\n\n**Phase 2 â€” Train a stolen model (replication)**\n\nThe attacker trains their own fresh model on this stolen dataset. Instead of the real labels (which they do not have), they use the victim's predictions as labels. The stolen model learns to reproduce the victim's decision-making.\n\n**The key insight:** The victim model is being used as an expensive labelling service, entirely against its owner's intentions.\n\n**We test three query budgets** â€” 100, 500, and 2000 queries â€” to show how the quality of the stolen model improves as more information is gathered."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EXTRACTION ATTACK AT THREE QUERY BUDGETS\n# =============================================================================\n#\n# For each query budget n_queries we:\n#\n# PHASE 1 â€” QUERY THE VICTIM:\n#   art_victim.predict(query_x)\n#     -> Returns an array of shape (n_queries, nb_classes)\n#     -> Each row is a probability distribution: [0.02, 0.91, 0.03, 0.02, 0.02]\n#     -> All values in a row sum to 1.0\n#\n#   np.argmax(victim_probs, axis=1)\n#     -> axis=1 means: look across each row (not down each column)\n#     -> argmax returns the INDEX of the highest value in each row\n#     -> [0.02, 0.91, 0.03, 0.02, 0.02] â†’ index 1 (class 1 has the highest prob)\n#     -> Output is a 1D array of integer class labels: the stolen labels\n#\n# PHASE 2 â€” TRAIN THE STOLEN MODEL:\n#   LogisticRegression().fit(query_x, fake_labels)\n#     -> A plain sklearn fit() call â€” no special ART parameters\n#     -> The model learns to map inputs to the victim's predicted classes\n#     -> It does NOT know the real ground-truth labels â€” only the victim's answers\n#\n# WHY NOT USE CopycatCNN's BUILT-IN training?\n#   CopycatCNN.extract() passes batch_size and nb_epochs to the stolen\n#   classifier's fit() method. These are PyTorch/neural network parameters\n#   that sklearn's LogisticRegression.fit() does not accept â€” causing a\n#   TypeError. Separating the query and training phases avoids this entirely\n#   and makes the attack steps clearer for learning.\n# =============================================================================\n\nnb_classes    = len(np.unique(y))\nquery_budgets = [100, 500, 2000]\nstolen_results = []\n\nprint(\"Running extraction attack at different query budgets...\")\nprint(\"=\" * 65)\nprint(f\"{'Budget':>8}  {'Queries used':>14}  {'Stolen accuracy':>16}  {'Agreement':>12}\")\nprint(\"-\" * 65)\n\nfor n_queries in query_budgets:\n\n    # ------------------------------------------------------------------\n    # PHASE 1: QUERY THE VICTIM API\n    # ------------------------------------------------------------------\n    # The attacker sends n_queries records to the victim API.\n    # art_victim.predict() simulates the API call and returns probabilities.\n    # ------------------------------------------------------------------\n\n    query_x     = X_steal_pool[:n_queries]\n    victim_probs = art_victim.predict(query_x)\n\n    # victim_probs shape: (n_queries, nb_classes)\n    # Each row: one probability per class, all rows sum to 1.0\n    assert victim_probs.shape == (n_queries, nb_classes)\n\n    # Convert probability distributions to hard class labels\n    # np.argmax across axis=1 picks the class index with highest probability\n    fake_labels = np.argmax(victim_probs, axis=1)\n\n    # fake_labels is now a 1D array of integers â€” the victim's predictions\n    # The attacker uses these as their training labels (stolen ground truth)\n\n    # ------------------------------------------------------------------\n    # PHASE 2: TRAIN THE STOLEN MODEL ON COLLECTED LABELS\n    # ------------------------------------------------------------------\n    # Fresh LogisticRegression for each budget (no carryover between runs).\n    # Logistic Regression finds a linear boundary between classes in\n    # feature space. It is simpler than the victim's Random Forest but\n    # can still approximate its behaviour given enough query-label pairs.\n    # max_iter=1000 allows enough optimisation steps to converge.\n    # ------------------------------------------------------------------\n\n    stolen_model = LogisticRegression(max_iter=1000, random_state=42)\n    stolen_model.fit(query_x, fake_labels)\n\n    # ------------------------------------------------------------------\n    # EVALUATE: HOW GOOD IS THE STOLEN COPY?\n    # ------------------------------------------------------------------\n    # We test on the neutral eval set â€” data neither model has seen.\n    #\n    # stolen_accuracy: how often stolen model gets the TRUE label right\n    # agreement      : how often stolen model matches VICTIM's prediction\n    #                  (these differ because victim itself is not perfect)\n    # ------------------------------------------------------------------\n\n    stolen_preds    = stolen_model.predict(X_eval)\n    stolen_accuracy = accuracy_score(y_eval,       stolen_preds)\n    victim_preds    = victim_model.predict(X_eval)\n    agreement       = accuracy_score(victim_preds,  stolen_preds)\n\n    stolen_results.append((n_queries, stolen_accuracy, agreement))\n    print(f\"{n_queries:>8}  {n_queries:>14,}  {stolen_accuracy*100:>15.2f}%  {agreement*100:>11.2f}%\")\n\nprint(\"-\" * 65)\nprint(f\"{'Victim':>8}  {'(benchmark)':>14}  {victim_accuracy*100:>15.2f}%  {'100.00%':>12}\")\nprint(\"\")\nprint(\"Accuracy  = how often the stolen model gets the TRUE label right\")\nprint(\"Agreement = how often stolen model matches the victim's prediction\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\nTwo metrics matter here:\n\n**Accuracy** measures how well the stolen model performs on the actual task â€” correctly classifying records into the 5 enrollment categories. This tells you how useful the stolen model is on its own.\n\n**Agreement** measures how closely the stolen model mimics the victim â€” even when the victim is wrong, does the stolen model make the same mistake? High agreement means the attacker successfully replicated the victim's decision-making, not just its correctness.\n\n**Notice the diminishing returns:** The jump from 100 to 500 queries likely gives a bigger improvement than the jump from 500 to 2000. Why? Because the first few hundred queries cover the main patterns in the data. Additional queries reveal increasingly rare edge cases that have smaller and smaller marginal value. This is a fundamental property of learning from data.\n\n**Also notice:** The stolen model uses LogisticRegression, not Random Forest like the victim. The attacker does not need to use the same model type â€” they just need a model that can reproduce the same input-output mapping.\n\n---\n\n## ðŸ“Š Step 4: Visualise the Trade-off"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# VISUALISE RESULTS\n# =============================================================================\n# Two charts side by side:\n#   Left  : stolen model accuracy vs query budget (compared to victim benchmark)\n#   Right : stolen model agreement with victim vs query budget\n#\n# The red dashed line on the left chart shows the victim's accuracy.\n# The attacker's goal is to get as close to that line as possible.\n# The red dashed line on the right chart shows perfect agreement (100%).\n# =============================================================================\n\nbudgets    = [r[0] for r in stolen_results]\naccuracies = [r[1]*100 for r in stolen_results]\nagreements = [r[2]*100 for r in stolen_results]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\n# Chart 1: Accuracy vs query budget\nax1.plot(budgets, accuracies, 'bo-', linewidth=2, markersize=8, label='Stolen model')\nax1.axhline(\n    y=victim_accuracy*100, color='red', linestyle='--', linewidth=1.5,\n    label=f'Victim benchmark ({victim_accuracy*100:.1f}%)'\n)\nax1.set_title('Stolen Model Accuracy vs Query Budget', fontsize=11, fontweight='bold')\nax1.set_xlabel('Number of Queries Sent to Victim API')\nax1.set_ylabel('Accuracy on Neutral Eval Set (%)')\nax1.legend()\nax1.grid(True, alpha=0.3)\nfor b, a in zip(budgets, accuracies):\n    ax1.annotate(f'{a:.1f}%', (b, a), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\n# Chart 2: Agreement vs query budget\nax2.plot(budgets, agreements, 'go-', linewidth=2, markersize=8, label='Stolen model')\nax2.axhline(\n    y=100, color='red', linestyle='--', linewidth=1.5, label='Perfect copy (100%)'\n)\nax2.set_title('Stolen Model Agreement with Victim vs Query Budget', fontsize=11, fontweight='bold')\nax2.set_xlabel('Number of Queries Sent to Victim API')\nax2.set_ylabel('Agreement with Victim Predictions (%)')\nax2.legend()\nax2.grid(True, alpha=0.3)\nfor b, a in zip(budgets, agreements):\n    ax2.annotate(f'{a:.1f}%', (b, a), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\nplt.tight_layout()\nplt.savefig('../outputs/lab4_extraction_results.png')\nplt.show()\nprint(\"Chart saved to outputs/lab4_extraction_results.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\n- The left chart shows how close the stolen model gets to the victim's benchmark accuracy as query budget grows.\n- The right chart shows how often the stolen model makes exactly the same decision as the victim.\n- Look at the shape of both curves â€” do they flatten out? That flattening is **diminishing returns** in action.\n\n### ðŸ§ª Try This\n\nAdd `50` as the first entry in `query_budgets` and rerun the attack.\n\n- Can you build a usable stolen model with just 50 queries?\n- At what query count would you say the attack is \"successful enough to be dangerous\"?\n- From a defender's side â€” if you were monitoring the API for suspicious behaviour, what query volume would trigger an alert?\n\n---\n\n## ðŸ›¡ï¸ Step 5: Think Like a Defender\n\nIf you were the company protecting the victim model, what would you do?\n\nThe three most common defenses are listed below. We demonstrate each one so you can see concretely what effect it has."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DEFENSIVE ANALYSIS\n# =============================================================================\n# Three practical defenses against model extraction attacks.\n# We demonstrate each one concretely so you can see what it changes.\n# =============================================================================\n\nprint(\"DEFENDER PERSPECTIVE â€” Three Defenses Against Extraction\")\nprint(\"=\" * 65)\nprint(\"\")\n\n# ------------------------------------------------------------------\n# DEFENSE 1: QUERY VOLUME MONITORING\n# ------------------------------------------------------------------\n# Track how many queries each user or IP address sends.\n# A legitimate user querying a credit scoring API might send 10-50\n# queries. Someone sending 2,000+ queries is behaving like an attacker.\n# Set a threshold and alert or block when it is exceeded.\n# ------------------------------------------------------------------\n\nprint(\"1. QUERY VOLUME MONITORING\")\nprint(\"-\" * 45)\nprint(\"   How many queries did each budget level need?\")\nfor n_queries, acc, agr in stolen_results:\n    print(f\"   {n_queries:>5,} queries â†’ {agr*100:.1f}% agreement with victim\")\nprint(\"\")\nprint(\"   A legitimate user rarely needs 500+ queries.\")\nprint(\"   Set an alert threshold. Block or rate-limit suspicious volumes.\")\nprint(\"   Recommendation: alert at 200 queries/day per user, block at 500.\")\nprint(\"\")\n\n# ------------------------------------------------------------------\n# DEFENSE 2: OUTPUT ROUNDING (REDUCING PRECISION)\n# ------------------------------------------------------------------\n# Returning full probability precision (e.g. 0.9143) gives the attacker\n# rich information to train their stolen model.\n# Rounding to 1 decimal place (e.g. 0.9) still tells the user which\n# class was predicted and roughly how confident the model is, but gives\n# the attacker much less to work with.\n#\n# np.round(exact_probs, 1) rounds each value to 1 decimal place.\n# The attacker gets coarser signal â†’ worse stolen model â†’ more queries needed.\n# ------------------------------------------------------------------\n\nprint(\"2. OUTPUT ROUNDING â€” REDUCING PROBABILITY PRECISION\")\nprint(\"-\" * 45)\nsample        = X_eval[:3]\nexact_probs   = victim_model.predict_proba(sample)\nrounded_probs = np.round(exact_probs, 1)\n\nprint(\"   Full precision (default â€” attacker gets maximum information):\")\nfor i, p in enumerate(exact_probs):\n    top3 = sorted(zip(p, range(len(p))), reverse=True)[:3]\n    print(f\"   Record {i+1}: {[f'{v:.4f}' for v, _ in top3]} (top 3 classes)\")\n\nprint(\"\")\nprint(\"   Rounded to 1 decimal (defender reduces information leak):\")\nfor i, p in enumerate(rounded_probs):\n    top3 = sorted(zip(p, range(len(p))), reverse=True)[:3]\n    print(f\"   Record {i+1}: {[f'{v:.1f}' for v, _ in top3]} (top 3 classes)\")\n\nprint(\"\")\nprint(\"   Less precision = less useful signal = attacker needs more queries\")\nprint(\"   Legitimate users still get a usable prediction.\")\nprint(\"\")\n\n# ------------------------------------------------------------------\n# DEFENSE 3: LABEL-ONLY RESPONSES\n# ------------------------------------------------------------------\n# The most aggressive defense: return ONLY the predicted class name,\n# no probabilities at all.\n# This strips all confidence information from the API response.\n# The attacker gets far less information per query, making the attack\n# much harder and requiring far more queries to reach the same quality.\n# The downside: some legitimate applications need confidence scores.\n# ------------------------------------------------------------------\n\nprint(\"3. LABEL-ONLY RESPONSES\")\nprint(\"-\" * 45)\nprint(\"   Full response (attacker gets probabilities):\")\nfor i, p in enumerate(exact_probs[:2]):\n    label = np.argmax(p)\n    print(f\"   Record {i+1}: class={label}  probs={[f'{v:.4f}' for v in p]}\")\n\nprint(\"\")\nprint(\"   Label-only response (attacker gets nothing but the decision):\")\nfor i, p in enumerate(exact_probs[:2]):\n    label = np.argmax(p)\n    print(f\"   Record {i+1}: class={label}\")\n\nprint(\"\")\nprint(\"   The attacker now gets only 1 bit of signal per query (which class).\")\nprint(\"   Reaching 80% agreement would require ~10x more queries.\")\nprint(\"   Legitimate users who only need a decision are unaffected.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ðŸ‘€ What Do You See?\n\n- **Query monitoring** is the easiest to implement and catches volume-based attacks quickly. But a sophisticated attacker can slow their queries down to stay under the threshold.\n- **Output rounding** degrades the attacker's signal without affecting the user experience much. It is a good default setting for any public API.\n- **Label-only** is the strongest defense but may break applications that depend on confidence scores for their own decision-making.\n\n**In practice, most real-world ML APIs combine all three** â€” rounding their outputs, monitoring query volume, and providing confidence scores only to verified enterprise customers.\n\n### ðŸ’­ Which defense would you implement first, and why?\n\n---\n\n## ðŸ’­ Step 6: Reflect on What You Have Learned"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# REFLECTION â€” edit your answers below and run to save them\n# =============================================================================\n\nreflection = \"\"\"\nLAB 4 - EXTRACTION ATTACK REFLECTION\n======================================\n\nQ1: In plain English, what is a model extraction attack?\n    What does the attacker end up with at the end, and why is that valuable?\nA1: [TYPE YOUR ANSWER HERE]\n\nQ2: The attacker never saw the victim model's code, weights, or training data.\n    They only had API access. What does this tell you about the security risk\n    of exposing machine learning models through public APIs?\nA2: [TYPE YOUR ANSWER HERE]\n\nQ3: We used LogisticRegression for the stolen model but the victim used\n    RandomForest. Does the stolen model need to be the same type?\n    What does this tell you about how the attacker works?\nA3: [TYPE YOUR ANSWER HERE]\n\nQ4: Explain diminishing returns in plain English in the context of this attack.\n    Why does each additional query give the attacker less new information?\nA4: [TYPE YOUR ANSWER HERE]\n\nQ5: Rank the three defenses (query monitoring, output rounding, label-only)\n    from most to least effective against a determined attacker.\n    Justify your ranking.\nA5: [TYPE YOUR ANSWER HERE]\n\nQ6: Looking back across all four labs, which attack do you think poses\n    the greatest risk to organisations deploying AI in high-stakes domains\n    (healthcare, finance, criminal justice)? Justify your answer.\nA6: [TYPE YOUR ANSWER HERE]\n\nBONUS: Can you think of a scenario where performing a model extraction attack\n       would be ethically justified? What safeguards would you put in place?\nBONUS: [TYPE YOUR ANSWER HERE]\n\"\"\"\n\nwith open('../outputs/Lab4_Reflection.txt', 'w') as f:\n    f.write(reflection)\n\nprint(\"Reflection saved to outputs/Lab4_Reflection.txt\")\nprint(reflection)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## âœ… Lab 4 Complete â€” And So Is the Course!\n\nYou have worked through all four core attack types:\n\n| Lab | Attack | When It Happens | What the Attacker Targets |\n|-----|--------|----------------|--------------------------|\n| 1 | Evasion | After deployment | Model inputs â€” crafting inputs that fool the model |\n| 2 | Poisoning | During training | Training data â€” corrupting what the model learns |\n| 3 | Inference | After deployment | Data privacy â€” figuring out who was in the training set |\n| 4 | Extraction | After deployment | Model IP â€” stealing the model through API queries |\n\nEach attack represents a real threat that AI security practitioners defend against today. The tools and techniques you used in this lab are the same ones used by researchers at IBM, Microsoft, Google, and security firms worldwide.\n\nThe Adversarial Robustness Toolbox (ART) that powered these labs is open source and actively maintained â€” it is the same library used to evaluate AI systems in production environments.\n\nReturn to [START_HERE.ipynb](START_HERE.ipynb) to review all four labs.\n\n---\n*Built with the Adversarial Robustness Toolbox (ART) â€” https://github.com/Trusted-AI/adversarial-robustness-toolbox*"
  }
 ]
}